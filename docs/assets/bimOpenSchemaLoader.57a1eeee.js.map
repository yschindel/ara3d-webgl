{"version":3,"file":"bimOpenSchemaLoader.57a1eeee.js","sources":["../../node_modules/hyparquet/src/constants.js","../../node_modules/hyparquet/src/wkb.js","../../node_modules/hyparquet/src/convert.js","../../node_modules/hyparquet/src/schema.js","../../node_modules/hyparquet/src/thrift.js","../../node_modules/hyparquet/src/geoparquet.js","../../node_modules/hyparquet/src/metadata.js","../../node_modules/hyparquet/src/utils.js","../../node_modules/hyparquet/src/filter.js","../../node_modules/hyparquet/src/plan.js","../../node_modules/hyparquet/src/assemble.js","../../node_modules/hyparquet/src/delta.js","../../node_modules/hyparquet/src/encoding.js","../../node_modules/hyparquet/src/plain.js","../../node_modules/hyparquet/src/snappy.js","../../node_modules/hyparquet/src/datapage.js","../../node_modules/hyparquet/src/column.js","../../node_modules/hyparquet/src/rowgroup.js","../../node_modules/hyparquet/src/read.js","../../src/loader/buildGeometryGroup.ts","../../src/loader/bimOpenSchemaLoader.ts"],"sourcesContent":["\n/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetTypes = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\n/** @type {import('../src/types.d.ts').Encoding[]} */\nexport const Encodings = [\n  'PLAIN',\n  'GROUP_VAR_INT', // deprecated\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\n/** @type {import('../src/types.d.ts').FieldRepetitionType[]} */\nexport const FieldRepetitionTypes = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedTypes = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').CompressionCodec[]} */\nexport const CompressionCodecs = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageTypes = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrders = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n\n/** @type {import('../src/types.d.ts').EdgeInterpolationAlgorithm[]} */\nexport const EdgeInterpolationAlgorithms = [\n  'SPHERICAL',\n  'VINCENTY',\n  'THOMAS',\n  'ANDOYER',\n  'KARNEY',\n]\n","/**\n * WKB (Well-Known Binary) decoder for geometry objects.\n *\n * @import {DataReader, Geometry} from '../src/types.js'\n * @param {DataReader} reader\n * @returns {Geometry} geometry object\n */\nexport function wkbToGeojson(reader) {\n  const flags = getFlags(reader)\n\n  if (flags.type === 1) { // Point\n    return { type: 'Point', coordinates: readPosition(reader, flags) }\n  } else if (flags.type === 2) { // LineString\n    return { type: 'LineString', coordinates: readLine(reader, flags) }\n  } else if (flags.type === 3) { // Polygon\n    return { type: 'Polygon', coordinates: readPolygon(reader, flags) }\n  } else if (flags.type === 4) { // MultiPoint\n    const points = []\n    for (let i = 0; i < flags.count; i++) {\n      points.push(readPosition(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPoint', coordinates: points }\n  } else if (flags.type === 5) { // MultiLineString\n    const lines = []\n    for (let i = 0; i < flags.count; i++) {\n      lines.push(readLine(reader, getFlags(reader)))\n    }\n    return { type: 'MultiLineString', coordinates: lines }\n  } else if (flags.type === 6) { // MultiPolygon\n    const polygons = []\n    for (let i = 0; i < flags.count; i++) {\n      polygons.push(readPolygon(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else if (flags.type === 7) { // GeometryCollection\n    const geometries = []\n    for (let i = 0; i < flags.count; i++) {\n      geometries.push(wkbToGeojson(reader))\n    }\n    return { type: 'GeometryCollection', geometries }\n  } else {\n    throw new Error(`Unsupported geometry type: ${flags.type}`)\n  }\n}\n\n/**\n * @typedef {object} WkbFlags\n * @property {boolean} littleEndian\n * @property {number} type\n * @property {number} dim\n * @property {number} count\n */\n\n/**\n * Extract ISO WKB flags and base geometry type.\n *\n * @param {DataReader} reader\n * @returns {WkbFlags}\n */\nfunction getFlags(reader) {\n  const { view } = reader\n  const littleEndian = view.getUint8(reader.offset++) === 1\n  const rawType = view.getUint32(reader.offset, littleEndian)\n  reader.offset += 4\n\n  const type = rawType % 1000\n  const flags = Math.floor(rawType / 1000)\n\n  let count = 0\n  if (type > 1 && type <= 7) {\n    count = view.getUint32(reader.offset, littleEndian)\n    reader.offset += 4\n  }\n\n  // XY, XYZ, XYM, XYZM\n  let dim = 2\n  if (flags) dim++\n  if (flags === 3) dim++\n\n  return { littleEndian, type, dim, count }\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[]}\n */\nfunction readPosition(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.dim; i++) {\n    const coord = reader.view.getFloat64(reader.offset, flags.littleEndian)\n    reader.offset += 8\n    points.push(coord)\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][]}\n */\nfunction readLine(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.count; i++) {\n    points.push(readPosition(reader, flags))\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][][]}\n */\nfunction readPolygon(reader, flags) {\n  const { view } = reader\n  const rings = []\n  for (let r = 0; r < flags.count; r++) {\n    const count = view.getUint32(reader.offset, flags.littleEndian)\n    reader.offset += 4\n    rings.push(readLine(reader, { ...flags, count }))\n  }\n  return rings\n}\n","import { wkbToGeojson } from './wkb.js'\n\n/**\n * @import {ColumnDecoder, DecodedArray, Encoding, ParquetParsers} from '../src/types.js'\n */\n\nconst decoder = new TextDecoder()\n\n/**\n * Default type parsers when no custom ones are given\n * @type ParquetParsers\n */\nexport const DEFAULT_PARSERS = {\n  timestampFromMilliseconds(millis) {\n    return new Date(Number(millis))\n  },\n  timestampFromMicroseconds(micros) {\n    return new Date(Number(micros / 1000n))\n  },\n  timestampFromNanoseconds(nanos) {\n    return new Date(Number(nanos / 1000000n))\n  },\n  dateFromDays(days) {\n    return new Date(days * 86400000)\n  },\n  stringFromBytes(bytes) {\n    return bytes && decoder.decode(bytes)\n  },\n  geometryFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n  geographyFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n}\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {Encoding} encoding\n * @param {ColumnDecoder} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, encoding, columnDecoder) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, columnDecoder)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {Pick<ColumnDecoder, \"element\" | \"utf8\" | \"parsers\">} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, columnDecoder) {\n  const { element, parsers, utf8 = true } = columnDecoder\n  const { type, converted_type: ctype, logical_type: ltype } = element\n  if (ctype === 'DECIMAL') {\n    const scale = element.scale || 0\n    const factor = 10 ** -scale\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[i] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (!ctype && type === 'INT96') {\n    return Array.from(data).map(v => parsers.timestampFromNanoseconds(parseInt96Nanos(v)))\n  }\n  if (ctype === 'DATE') {\n    return Array.from(data).map(v => parsers.dateFromDays(v))\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    return Array.from(data).map(v => parsers.timestampFromMilliseconds(v))\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    return Array.from(data).map(v => parsers.timestampFromMicroseconds(v))\n  }\n  if (ctype === 'JSON') {\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ltype?.type === 'GEOMETRY') {\n    return data.map(v => parsers.geometryFromBytes(v))\n  }\n  if (ltype?.type === 'GEOGRAPHY') {\n    return data.map(v => parsers.geographyFromBytes(v))\n  }\n  if (ctype === 'UTF8' || ltype?.type === 'STRING' || utf8 && type === 'BYTE_ARRAY') {\n    return data.map(v => parsers.stringFromBytes(v))\n  }\n  if (ctype === 'UINT_64' || ltype?.type === 'INTEGER' && ltype.bitWidth === 64 && !ltype.isSigned) {\n    if (data instanceof BigInt64Array) {\n      return new BigUint64Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = BigInt(data[i])\n    return arr\n  }\n  if (ctype === 'UINT_32' || ltype?.type === 'INTEGER' && ltype.bitWidth === 32 && !ltype.isSigned) {\n    if (data instanceof Int32Array) {\n      return new Uint32Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new Uint32Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = data[i]\n    return arr\n  }\n  if (ltype?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (ltype?.type === 'TIMESTAMP') {\n    const { unit } = ltype\n    /** @type {ParquetParsers[keyof ParquetParsers]} */\n    let parser = parsers.timestampFromMilliseconds\n    if (unit === 'MICROS') parser = parsers.timestampFromMicroseconds\n    if (unit === 'NANOS') parser = parsers.timestampFromNanoseconds\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parser(data[i])\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  if (!bytes.length) return 0\n\n  let value = 0n\n  for (const byte of bytes) {\n    value = value * 256n + BigInt(byte)\n  }\n\n  // handle signed\n  const bits = bytes.length * 8\n  if (value >= 2n ** BigInt(bits - 1)) {\n    value -= 2n ** BigInt(bits)\n  }\n\n  return Number(value)\n}\n\n/**\n * Converts INT96 date format (hi 32bit days, lo 64bit nanos) to nanos since epoch\n * @param {bigint} value\n * @returns {bigint}\n */\nfunction parseInt96Nanos(value) {\n  const days = (value >> 64n) - 2440588n\n  const nano = value & 0xffffffffffffffffn\n  return days * 86400000000000n + nano\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * 2 ** -14 * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * 2 ** (exp - 15) * (1 + frac / 1024)\n}\n","/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get all physical (leaf) column names.\n *\n * @param {SchemaTree} schemaTree\n * @returns {string[]} list of physical column names\n */\nexport function getPhysicalColumns(schemaTree) {\n  /** @type {string[]} */\n  const columns = []\n  /** @param {SchemaTree} node */\n  function traverse(node) {\n    if (node.children.length) {\n      for (const child of node.children) {\n        traverse(child)\n      }\n    } else {\n      columns.push(node.element.name)\n    }\n  }\n  traverse(schemaTree)\n  return columns\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Returns true if a column is non-nested.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {boolean}\n */\nexport function isFlatColumn(schemaPath) {\n  if (schemaPath.length !== 2) return false\n  const [, column] = schemaPath\n  if (column.element.repetition_type === 'REPEATED') return false\n  if (column.children.length) return false\n  return true\n}\n","// TCompactProtocol types\nexport const CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @param {DataReader} reader\n * @returns {{ [key: `field_${number}`]: any }}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {ThriftObject} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @import {DataReader, ThriftObject, ThriftType} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} type\n * @returns {ThriftType}\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const byte = reader.view.getUint8(reader.offset++)\n    const elemType = byte & 0x0f\n    let listSize = byte >> 4\n    if (listSize === 15) {\n      listSize = readVarInt(reader)\n    }\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {ThriftObject} */\n    const structValues = {}\n    let lastFid = 0\n    while (true) {\n      const [fieldType, fid, newLastFid] = readFieldBegin(reader, lastFid)\n      lastFid = newLastFid\n      if (fieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${fid}`] = readElement(reader, fieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP, SET, UUID\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int aka Unsigned LEB128.\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> 1n ^ -(zigzag & 1n)\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const byte = reader.view.getUint8(reader.offset++)\n  const type = byte & 0x0f\n  if (type === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = byte >> 4\n  const fid = delta ? lastFid + delta : readZigZag(reader)\n  return [type, fid, fid]\n}\n","/**\n * @import {KeyValue, LogicalType, SchemaElement} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {KeyValue[] | undefined} key_value_metadata\n * @returns {void}\n */\nexport function markGeoColumns(schema, key_value_metadata) {\n  // Prepare the list of GeoParquet columns\n  /** @type {Map<string, LogicalType>} */\n  const columns = new Map()\n  const geo = key_value_metadata?.find(({ key }) => key === 'geo')?.value\n  const decodedColumns = (geo && JSON.parse(geo)?.columns) ?? {}\n  for (const [name, column] of Object.entries(decodedColumns)) {\n    if (column.encoding !== 'WKB') {\n      continue\n    }\n    const type = column.edges === 'spherical' ? 'GEOGRAPHY' : 'GEOMETRY'\n    const id = column.crs?.id ?? column.crs?.ids?.[0]\n    const crs = id ? `${id.authority}:${id.code.toString()}` : undefined\n    // Note: we can't infer GEOGRAPHY's algorithm from GeoParquet\n    columns.set(name, { type, crs })\n  }\n\n  // Mark schema elements with logical type\n  // Only look at root-level columns of type BYTE_ARRAY without existing logical_type\n  for (let i = 1; i < schema.length; i++) { // skip root\n    const element = schema[i]\n    const { logical_type, name, num_children, repetition_type, type } = element\n    if (num_children) {\n      i += num_children\n      continue // skip the element and its children\n    }\n    if (type === 'BYTE_ARRAY' && logical_type === undefined && repetition_type !== 'REPEATED') {\n      element.logical_type = columns.get(name)\n    }\n  }\n}\n","import { CompressionCodecs, ConvertedTypes, EdgeInterpolationAlgorithms, Encodings, FieldRepetitionTypes, PageTypes, ParquetTypes } from './constants.js'\nimport { DEFAULT_PARSERS, parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\nimport { markGeoColumns } from './geoparquet.js'\n\nexport const defaultInitialFetchSize = 1 << 19 // 512kb\n\nconst decoder = new TextDecoder()\nfunction decode(/** @type {Uint8Array} */ value) {\n  return value && decoder.decode(value)\n}\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {MetadataOptions & { initialFetchSize?: number }} options initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, { parsers, initialFetchSize = defaultInitialFetchSize, geoparquet = true } = {}) {\n  if (!asyncBuffer || !(asyncBuffer.byteLength >= 0)) throw new Error('parquet expected AsyncBuffer')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer, { parsers, geoparquet })\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer, { parsers, geoparquet })\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @import {KeyValue} from '../src/types.d.ts'\n * @param {ArrayBuffer} arrayBuffer parquet file footer\n * @param {MetadataOptions} options metadata parsing options\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer, { parsers, geoparquet = true } = {}) {\n  if (!(arrayBuffer instanceof ArrayBuffer)) throw new Error('parquet expected ArrayBuffer')\n  const view = new DataView(arrayBuffer)\n\n  // Use default parsers if not given\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetTypes[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionTypes[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedTypes[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetTypes[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encodings[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodecs[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8?.map((/** @type {any} */ kv) => ({\n          key: decode(kv.field_1),\n          value: decode(kv.field_2),\n        })),\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex], parsers),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageTypes[encodingStat.field_1],\n          encoding: Encodings[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n        geospatial_statistics: column.field_3.field_17 && {\n          bbox: column.field_3.field_17.field_1 && {\n            xmin: column.field_3.field_17.field_1.field_1,\n            xmax: column.field_3.field_17.field_1.field_2,\n            ymin: column.field_3.field_17.field_1.field_3,\n            ymax: column.field_3.field_17.field_1.field_4,\n            zmin: column.field_3.field_17.field_1.field_5,\n            zmax: column.field_3.field_17.field_1.field_6,\n            mmin: column.field_3.field_17.field_1.field_7,\n            mmax: column.field_3.field_17.field_1.field_8,\n          },\n          geospatial_types: column.field_3.field_17.field_2,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_8,\n      encrypted_column_metadata: column.field_9,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  /** @type {KeyValue[] | undefined} */\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ kv) => ({\n    key: decode(kv.field_1),\n    value: decode(kv.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  if (geoparquet) {\n    markGeoColumns(schema, key_value_metadata)\n  }\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {{schema: SchemaElement[]}} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema({ schema }) {\n  return getSchemaPath(schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  if (logicalType?.field_16) return {\n    type: 'VARIANT',\n    specification_version: logicalType.field_16.field_1,\n  }\n  if (logicalType?.field_17) return {\n    type: 'GEOMETRY',\n    crs: decode(logicalType.field_17.field_1),\n  }\n  if (logicalType?.field_18) return {\n    type: 'GEOGRAPHY',\n    crs: decode(logicalType.field_18.field_1),\n    algorithm: EdgeInterpolationAlgorithms[logicalType.field_18.field_2],\n  }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MetadataOptions, MinMaxType, ParquetParsers, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema, parsers) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema, parsers),\n    min: convertMetadata(stats.field_2, schema, parsers),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema, parsers),\n    min_value: convertMetadata(stats.field_6, schema, parsers),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema, parsers) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return parsers.stringFromBytes(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return parsers.dateFromDays(view.getInt32(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return parsers.timestampFromNanoseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * 10 ** -(schema.scale || 0)\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n","import { defaultInitialFetchSize } from './metadata.js'\n\n/**\n * Replace bigint, date, etc with legal JSON types.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa first array\n * @param {DecodedArray} bbb second array\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Deep equality comparison\n *\n * @param {any} a First object to compare\n * @param {any} b Second object to compare\n * @returns {boolean} true if objects are equal\n */\nexport function equals(a, b) {\n  if (a === b) return true\n  if (a instanceof Uint8Array && b instanceof Uint8Array) return equals(Array.from(a), Array.from(b))\n  if (!a || !b || typeof a !== typeof b) return false\n  return Array.isArray(a) && Array.isArray(b)\n    ? a.length === b.length && a.every((v, i) => equals(v, b[i]))\n    : typeof a === 'object' && Object.keys(a).length === Object.keys(b).length && Object.keys(a).every(k => equals(a[k], b[k]))\n}\n\n/**\n * Get the byte length using fetch with a ranged GET request.\n * Aborts the request if server returns 200 instead of 206.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [fetchFn] fetch function to use\n * @returns {Promise<number>}\n */\nasync function byteLengthFromUrlUsingFetch(url, requestInit = {}, fetchFn = globalThis.fetch) {\n  const controller = new AbortController()\n  const headers = new Headers(requestInit.headers)\n  headers.set('Range', 'bytes=0-0')\n\n  const res = await fetchFn(url, {\n    ...requestInit,\n    headers,\n    signal: controller.signal,\n  })\n\n  if (!res.ok) throw new Error(`fetch with range failed ${res.status}`)\n\n  // Server supports Range requests (206 Partial Content)\n  if (res.status === 206) {\n    const contentRange = res.headers.get('Content-Range')\n    if (!contentRange) throw new Error('missing content-range header')\n\n    // Parse \"bytes 0-0/9446073\" to get total length\n    const match = contentRange.match(/bytes \\d+-\\d+\\/(\\d+)/)\n    if (!match) throw new Error(`invalid content-range header: ${contentRange}`)\n\n    return parseInt(match[1])\n  }\n\n  // Server ignored Range and returned 200 - get Content-Length and abort request\n  if (res.status === 200) {\n    const contentLength = res.headers.get('Content-Length')\n\n    // Abort the request to stop any ongoing download\n    controller.abort()\n\n    if (contentLength) return parseInt(contentLength)\n  }\n\n  throw new Error('server does not support range requests and missing content-length')\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If HEAD fails with 403 (e.g., with signed S3 URLs), falls back to a ranged GET request.\n * If HEAD succeeds but Content-Length is missing, falls back to GET with range.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [customFetch] fetch function to use\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit, customFetch) {\n  const fetch = customFetch ?? globalThis.fetch\n  const res = await fetch(url, { ...requestInit, method: 'HEAD' })\n\n  // If HEAD request is forbidden (common with signed S3 URLs), try GET with range\n  if (res.status === 403) {\n    return byteLengthFromUrlUsingFetch(url, requestInit, fetch)\n  }\n\n  if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n  const length = res.headers.get('Content-Length')\n  // If Content-Length is missing from HEAD, fallback to GET with range\n  if (!length) {\n    return byteLengthFromUrlUsingFetch(url, requestInit, fetch)\n  }\n  return parseInt(length)\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If fetch is provided, it will be used instead of the global fetch.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {typeof globalThis.fetch} [options.fetch] fetch function to use\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit, fetch: customFetch }) {\n  if (!url) throw new Error('missing url')\n  const fetch = customFetch ?? globalThis.fetch\n  // byte length from HEAD request\n  byteLength ??= await byteLengthFromUrl(url, requestInit, fetch)\n\n  /**\n   * A promise for the whole buffer, if range requests are not supported.\n   * @type {Promise<ArrayBuffer>|undefined}\n   */\n  let buffer = undefined\n  const init = requestInit || {}\n\n  return {\n    byteLength,\n    async slice(start, end) {\n      if (buffer) {\n        return buffer.then(buffer => buffer.slice(start, end))\n      }\n\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n\n      if (res.status === 200) {\n        // Endpoint does not support range requests and returned the whole object\n        buffer = res.arrayBuffer()\n        return buffer.then(buffer => buffer.slice(start, end))\n      } else if (res.status === 206) {\n        // The endpoint supports range requests and sent us the requested range\n        return res.arrayBuffer()\n      } else {\n        throw new Error(`fetch received unexpected status code ${res.status}`)\n      }\n    },\n  }\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @param {{ minSize?: number }} [options]\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }, { minSize = defaultInitialFetchSize } = {}) {\n  if (byteLength < minSize) {\n    // Cache whole file if it's small\n    const buffer = slice(0, byteLength)\n    return {\n      byteLength,\n      async slice(start, end) {\n        return (await buffer).slice(start, end)\n      },\n    }\n  }\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n\n/**\n * Flatten a list of lists into a single list.\n *\n * @param {DecodedArray[]} [chunks]\n * @returns {DecodedArray}\n */\nexport function flatten(chunks) {\n  if (!chunks) return []\n  if (chunks.length === 1) return chunks[0]\n  /** @type {any[]} */\n  const output = []\n  for (const chunk of chunks) {\n    concat(output, chunk)\n  }\n  return output\n}\n","import { equals } from './utils.js'\n\n/**\n * Match a record against a query filter\n *\n * @param {Record<string, any>} record\n * @param {ParquetQueryFilter} filter\n * @returns {boolean}\n * @example matchQuery({ id: 1 }, { id: {$gte: 1} }) // true\n */\nexport function matchFilter(record, filter = {}) {\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    return filter.$and.every(subQuery => matchFilter(record, subQuery))\n  }\n  if ('$or' in filter && Array.isArray(filter.$or)) {\n    return filter.$or.some(subQuery => matchFilter(record, subQuery))\n  }\n  if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    return !filter.$nor.some(subQuery => matchFilter(record, subQuery))\n  }\n\n  return Object.entries(filter).every(([field, condition]) => {\n    const value = record[field]\n\n    // implicit $eq for non-object conditions\n    if (typeof condition !== 'object' || condition === null || Array.isArray(condition)) {\n      return equals(value, condition)\n    }\n\n    return Object.entries(condition || {}).every(([operator, target]) => {\n      if (operator === '$gt') return value > target\n      if (operator === '$gte') return value >= target\n      if (operator === '$lt') return value < target\n      if (operator === '$lte') return value <= target\n      if (operator === '$eq') return equals(value, target)\n      if (operator === '$ne') return !equals(value, target)\n      if (operator === '$in') return Array.isArray(target) && target.includes(value)\n      if (operator === '$nin') return Array.isArray(target) && !target.includes(value)\n      if (operator === '$not') return !matchFilter({ [field]: value }, { [field]: target })\n      return true\n    })\n  })\n}\n\n/**\n * Check if a row group can be skipped based on filter and column statistics.\n *\n * @import {ParquetQueryFilter, RowGroup} from '../src/types.js'\n * @param {ParquetQueryFilter | undefined} filter\n * @param {RowGroup} group\n * @param {string[]} physicalColumns\n * @returns {boolean} true if the row group can be skipped\n */\nexport function canSkipRowGroup(filter, group, physicalColumns) {\n  if (!filter) return false\n\n  // Handle logical operators\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    // For AND, we can skip if ANY condition allows skipping\n    return filter.$and.some(subFilter => canSkipRowGroup(subFilter, group, physicalColumns))\n  }\n  if ('$or' in filter && Array.isArray(filter.$or)) {\n    // For OR, we can skip only if ALL conditions allow skipping\n    return filter.$or.every(subFilter => canSkipRowGroup(subFilter, group, physicalColumns))\n  }\n  if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    // For NOR, we can skip if none of the conditions allow skipping\n    // This is complex, so we'll be conservative and not skip\n    return false\n  }\n\n  // Check column filters\n  for (const [field, condition] of Object.entries(filter)) {\n    // Find the column chunk for this field\n    const columnIndex = physicalColumns.indexOf(field)\n    if (columnIndex === -1) continue\n\n    const columnChunk = group.columns[columnIndex]\n    const stats = columnChunk.meta_data?.statistics\n    if (!stats) continue // No statistics available, can't skip\n\n    const { min, max, min_value, max_value } = stats\n    const minVal = min_value !== undefined ? min_value : min\n    const maxVal = max_value !== undefined ? max_value : max\n\n    if (minVal === undefined || maxVal === undefined) continue\n\n    // Handle operators\n    for (const [operator, target] of Object.entries(condition || {})) {\n      if (operator === '$gt' && maxVal <= target) return true\n      if (operator === '$gte' && maxVal < target) return true\n      if (operator === '$lt' && minVal >= target) return true\n      if (operator === '$lte' && minVal > target) return true\n      if (operator === '$eq' && (target < minVal || target > maxVal)) return true\n      if (operator === '$ne' && equals(minVal, maxVal) && equals(minVal, target)) return true\n      if (operator === '$in' && Array.isArray(target) && target.every(v => v < minVal || v > maxVal)) return true\n      if (operator === '$nin' && Array.isArray(target) && equals(minVal, maxVal) && target.includes(minVal)) return true\n    }\n  }\n\n  return false\n}\n","import { canSkipRowGroup } from './filter.js'\nimport { parquetSchema } from './metadata.js'\nimport { getPhysicalColumns } from './schema.js'\nimport { concat } from './utils.js'\n\n// Combine column chunks into a single byte range if less than 32mb\nconst columnChunkAggregation = 1 << 25 // 32mb\n\n/**\n * @import {AsyncBuffer, ByteRange, ColumnMetaData, GroupPlan, ParquetReadOptions, QueryPlan} from '../src/types.js'\n */\n/**\n * Plan which byte ranges to read to satisfy a read request.\n * Metadata must be non-null.\n *\n * @param {ParquetReadOptions} options\n * @returns {QueryPlan}\n */\nexport function parquetPlan({ metadata, rowStart = 0, rowEnd = Infinity, columns, filter }) {\n  if (!metadata) throw new Error('parquetPlan requires metadata')\n  /** @type {GroupPlan[]} */\n  const groups = []\n  /** @type {ByteRange[]} */\n  const fetches = []\n  const physicalColumns = getPhysicalColumns(parquetSchema(metadata))\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    const groupRows = Number(rowGroup.num_rows)\n    const groupEnd = groupStart + groupRows\n    // if row group overlaps with row range, add it to the plan\n    if (groupRows > 0 && groupEnd > rowStart && groupStart < rowEnd && !canSkipRowGroup(filter, rowGroup, physicalColumns)) {\n      /** @type {ByteRange[]} */\n      const ranges = []\n      // loop through each column chunk\n      for (const { file_path, meta_data } of rowGroup.columns) {\n        if (file_path) throw new Error('parquet file_path not supported')\n        if (!meta_data) throw new Error('parquet column metadata is undefined')\n        // add included columns to the plan\n        if (!columns || columns.includes(meta_data.path_in_schema[0])) {\n          ranges.push(getColumnRange(meta_data))\n        }\n      }\n      const selectStart = Math.max(rowStart - groupStart, 0)\n      const selectEnd = Math.min(rowEnd - groupStart, groupRows)\n      groups.push({ ranges, rowGroup, groupStart, groupRows, selectStart, selectEnd })\n\n      // map group plan to ranges\n      const groupSize = ranges[ranges.length - 1]?.endByte - ranges[0]?.startByte\n      if (!columns && groupSize < columnChunkAggregation) {\n        // full row group\n        fetches.push({\n          startByte: ranges[0].startByte,\n          endByte: ranges[ranges.length - 1].endByte,\n        })\n      } else if (ranges.length) {\n        concat(fetches, ranges)\n      } else if (columns?.length) {\n        throw new Error(`parquet columns not found: ${columns.join(', ')}`)\n      }\n    }\n\n    groupStart = groupEnd\n  }\n  if (!isFinite(rowEnd)) rowEnd = groupStart\n\n  return { metadata, rowStart, rowEnd, columns, fetches, groups }\n}\n\n/**\n * @param {ColumnMetaData} columnMetadata\n * @returns {ByteRange}\n */\nexport function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {\n  const columnOffset = dictionary_page_offset || data_page_offset\n  return {\n    startByte: Number(columnOffset),\n    endByte: Number(columnOffset + total_compressed_size),\n  }\n}\n\n/**\n * Prefetch byte ranges from an AsyncBuffer.\n *\n * @param {AsyncBuffer} file\n * @param {QueryPlan} plan\n * @returns {AsyncBuffer}\n */\nexport function prefetchAsyncBuffer(file, { fetches }) {\n  // fetch byte ranges from the file\n  const promises = fetches.map(({ startByte, endByte }) => file.slice(startByte, endByte))\n  return {\n    byteLength: file.byteLength,\n    slice(start, end = file.byteLength) {\n      // find matching slice\n      const index = fetches.findIndex(({ startByte, endByte }) => startByte <= start && end <= endByte)\n      if (index < 0) throw new Error(`no prefetch for range [${start}, ${end}]`)\n      if (fetches[index].startByte !== start || fetches[index].endByte !== end) {\n        // slice a subrange of the prefetch\n        const startOffset = start - fetches[index].startByte\n        const endOffset = end - fetches[index].startByte\n        if (promises[index] instanceof Promise) {\n          return promises[index].then(buffer => buffer.slice(startOffset, endOffset))\n        } else {\n          return promises[index].slice(startOffset, endOffset)\n        }\n      } else {\n        return promises[index]\n      }\n    },\n  }\n}\n","import { getMaxDefinitionLevel, isListLike, isMapLike } from './schema.js'\n\n/**\n * Reconstructs a complex nested structure from flat arrays of values and\n * definition and repetition levels, according to Dremel encoding.\n *\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {SchemaTree[]} schemaPath\n * @returns {DecodedArray}\n */\nexport function assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  if (!n) return values\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        // go into last list\n        currentContainer = currentContainer.at(-1)\n        containerStack.push(currentContainer)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @param {Map<string, DecodedArray>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @import {DecodedArray, SchemaTree} from '../src/types.d.ts'\n * @param {DecodedArray} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {DecodedArray} keys\n * @param {DecodedArray} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n","import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n","import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - bitwidth\n * @param {DecodedArray} output\n * @param {number} [length] - length of the encoded data\n */\nexport function readRleBitPackedHybrid(reader, width, output, length) {\n  if (length === undefined) {\n    length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  const startOffset = reader.offset\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  reader.offset = startOffset + length // duckdb writes an empty block\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n","/**\n * Read `count` values of the given type from the reader.view.\n *\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n","/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      copyBytes(output, outPos - offset, output, outPos, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n","import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, { type, element, schemaPath }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    dataPage = readPlain(reader, type, nValues, element.type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      if (type === 'BOOLEAN') {\n        readRleBitPackedHybrid(reader, bitWidth, dataPage)\n        dataPage = dataPage.map(x => !!x) // convert to boolean\n      } else {\n        // assert(daph.encoding.endsWith('_DICTIONARY'))\n        readRleBitPackedHybrid(reader, bitWidth, dataPage, view.byteLength - reader.offset)\n      }\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else if (daph.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(reader, nValues, dataPage)\n  } else if (daph.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(reader, nValues, dataPage)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @import {ColumnDecoder, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, columnDecoder) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    dataPage = readPlain(pageReader, type, nValues, element.type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, dataPage, uncompressedPageSize - 1)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(pageReader, nValues, type, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values, daph2.repetition_levels_byte_length)\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), values, daph2.definition_levels_byte_length)\n    return values\n  }\n}\n","import { assembleLists } from './assemble.js'\nimport { Encodings, PageTypes } from './constants.js'\nimport { convert, convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2 } from './datapage.js'\nimport { readPlain } from './plain.js'\nimport { isFlatColumn } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {RowGroupSelect} rowGroupSelect row group selection\n * @param {ColumnDecoder} columnDecoder column decoder params\n * @param {(chunk: SubColumnData) => void} [onPage] callback for each page\n * @returns {DecodedArray[]}\n */\nexport function readColumn(reader, { groupStart, selectStart, selectEnd }, columnDecoder, onPage) {\n  const { pathInSchema, schemaPath } = columnDecoder\n  const isFlat = isFlatColumn(schemaPath)\n  /** @type {DecodedArray[]} */\n  const chunks = []\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {DecodedArray | undefined} */\n  let lastChunk = undefined\n  let rowCount = 0\n\n  const emitLastChunk = onPage && (() => {\n    lastChunk && onPage({\n      pathInSchema,\n      columnData: lastChunk,\n      rowStart: groupStart + rowCount - lastChunk.length,\n      rowEnd: groupStart + rowCount,\n    })\n  })\n\n  while (isFlat ? rowCount < selectEnd : reader.offset < reader.view.byteLength - 1) {\n    if (reader.offset >= reader.view.byteLength - 1) break // end of reader\n\n    // read page header\n    const header = parquetHeader(reader)\n    if (header.type === 'DICTIONARY_PAGE') {\n      // assert(!dictionary)\n      dictionary = readPage(reader, header, columnDecoder, dictionary, undefined, 0)\n      dictionary = convert(dictionary, columnDecoder)\n    } else {\n      const lastChunkLength = lastChunk?.length || 0\n      const values = readPage(reader, header, columnDecoder, dictionary, lastChunk, selectStart - rowCount)\n      if (lastChunk === values) {\n        // continued from previous page\n        rowCount += values.length - lastChunkLength\n      } else {\n        emitLastChunk?.()\n        chunks.push(values)\n        rowCount += values.length\n        lastChunk = values\n      }\n    }\n  }\n  emitLastChunk?.()\n  // assert(rowCount >= selectEnd)\n  if (rowCount > selectEnd && lastChunk) {\n    // truncate last chunk to row limit\n    chunks[chunks.length - 1] = lastChunk.slice(0, selectEnd - (rowCount - lastChunk.length))\n  }\n  return chunks\n}\n\n/**\n * Read a page (data or dictionary) from a buffer.\n *\n * @param {DataReader} reader\n * @param {PageHeader} header\n * @param {ColumnDecoder} columnDecoder\n * @param {DecodedArray | undefined} dictionary\n * @param {DecodedArray | undefined} previousChunk\n * @param {number} pageStart skip this many rows in the page\n * @returns {DecodedArray}\n */\nexport function readPage(reader, header, columnDecoder, dictionary, previousChunk, pageStart) {\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  // read compressed_page_size bytes\n  const compressedBytes = new Uint8Array(\n    reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n  )\n  reader.offset += header.compressed_page_size\n\n  // parse page data by type\n  if (header.type === 'DATA_PAGE') {\n    const daph = header.data_page_header\n    if (!daph) throw new Error('parquet data page header is undefined')\n\n    // skip unnecessary non-nested pages\n    if (pageStart > daph.num_values && isFlatColumn(schemaPath)) {\n      return new Array(daph.num_values) // TODO: don't allocate array\n    }\n\n    const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), codec, compressors)\n    const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, columnDecoder)\n    // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n    // convert types, dereference dictionary, and assemble lists\n    let values = convertWithDictionary(dataPage, dictionary, daph.encoding, columnDecoder)\n    if (repetitionLevels.length || definitionLevels?.length) {\n      const output = Array.isArray(previousChunk) ? previousChunk : []\n      return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n    } else {\n      // wrap nested flat data by depth\n      for (let i = 2; i < schemaPath.length; i++) {\n        if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n          values = Array.from(values, e => [e])\n        }\n      }\n      return values\n    }\n  } else if (header.type === 'DATA_PAGE_V2') {\n    const daph2 = header.data_page_header_v2\n    if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n    // skip unnecessary pages\n    if (pageStart > daph2.num_rows) {\n      return new Array(daph2.num_values) // TODO: don't allocate array\n    }\n\n    const { definitionLevels, repetitionLevels, dataPage } =\n      readDataPageV2(compressedBytes, header, columnDecoder)\n\n    // convert types, dereference dictionary, and assemble lists\n    const values = convertWithDictionary(dataPage, dictionary, daph2.encoding, columnDecoder)\n    const output = Array.isArray(previousChunk) ? previousChunk : []\n    return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n  } else if (header.type === 'DICTIONARY_PAGE') {\n    const diph = header.dictionary_page_header\n    if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n    const page = decompressPage(\n      compressedBytes, Number(header.uncompressed_page_size), codec, compressors\n    )\n\n    const reader = { view: new DataView(page.buffer, page.byteOffset, page.byteLength), offset: 0 }\n    return readPlain(reader, type, diph.num_values, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported page type: ${header.type}`)\n  }\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnDecoder, DataReader, DecodedArray, PageHeader, RowGroupSelect, SubColumnData} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {PageHeader}\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageTypes[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encodings[header.field_5.field_2],\n    definition_level_encoding: Encodings[header.field_5.field_3],\n    repetition_level_encoding: Encodings[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encodings[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encodings[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n","import { assembleNested } from './assemble.js'\nimport { readColumn } from './column.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { getColumnRange } from './plan.js'\nimport { getSchemaPath } from './schema.js'\nimport { flatten } from './utils.js'\n\n/**\n * @import {AsyncColumn, AsyncRowGroup, DecodedArray, GroupPlan, ParquetParsers, ParquetReadOptions, QueryPlan, RowGroup, SchemaTree} from './types.js'\n */\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options\n * @param {QueryPlan} plan\n * @param {GroupPlan} groupPlan\n * @returns {AsyncRowGroup} resolves to column data\n */\nexport function readRowGroup(options, { metadata, columns }, groupPlan) {\n  const { file, compressors, utf8 } = options\n\n  /** @type {AsyncColumn[]} */\n  const asyncColumns = []\n  /** @type {ParquetParsers} */\n  const parsers = { ...DEFAULT_PARSERS, ...options.parsers }\n\n  // read column data\n  for (const { file_path, meta_data } of groupPlan.rowGroup.columns) {\n    if (file_path) throw new Error('parquet file_path not supported')\n    if (!meta_data) throw new Error('parquet column metadata is undefined')\n\n    // skip columns that are not requested\n    const columnName = meta_data.path_in_schema[0]\n    if (columns && !columns.includes(columnName)) continue\n\n    const { startByte, endByte } = getColumnRange(meta_data)\n    const columnBytes = endByte - startByte\n\n    // skip columns larger than 1gb\n    // TODO: stream process the data, returning only the requested rows\n    if (columnBytes > 1 << 30) {\n      console.warn(`parquet skipping huge column \"${meta_data.path_in_schema}\" ${columnBytes} bytes`)\n      // TODO: set column to new Error('parquet column too large')\n      continue\n    }\n\n    // wrap awaitable to ensure it's a promise\n    /** @type {Promise<ArrayBuffer>} */\n    const buffer = Promise.resolve(file.slice(startByte, endByte))\n\n    // read column data async\n    asyncColumns.push({\n      pathInSchema: meta_data.path_in_schema,\n      data: buffer.then(arrayBuffer => {\n        const schemaPath = getSchemaPath(metadata.schema, meta_data.path_in_schema)\n        const reader = { view: new DataView(arrayBuffer), offset: 0 }\n        const columnDecoder = {\n          pathInSchema: meta_data.path_in_schema,\n          type: meta_data.type,\n          element: schemaPath[schemaPath.length - 1].element,\n          schemaPath,\n          codec: meta_data.codec,\n          parsers,\n          compressors,\n          utf8,\n        }\n        return readColumn(reader, groupPlan, columnDecoder, options.onPage)\n      }),\n    })\n  }\n\n  return { groupStart: groupPlan.groupStart, groupRows: groupPlan.groupRows, asyncColumns }\n}\n\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object'} rowFormat\n * @returns {Promise<Record<string, any>[]>} resolves to row data\n */\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'array'} [rowFormat]\n * @returns {Promise<any[][]>} resolves to row data\n */\n/**\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object' | 'array'} [rowFormat]\n * @returns {Promise<Record<string, any>[] | any[][]>} resolves to row data\n */\nexport async function asyncGroupToRows({ asyncColumns }, selectStart, selectEnd, columns, rowFormat) {\n  // columnData[i] for asyncColumns[i]\n  // TODO: do it without flatten\n  const columnDatas = await Promise.all(asyncColumns.map(({ data }) => data.then(flatten)))\n\n  // careful mapping of column order for rowFormat: array\n  const includedColumnNames = asyncColumns\n    .map(child => child.pathInSchema[0])\n    .filter(name => !columns || columns.includes(name))\n  const columnOrder = columns ?? includedColumnNames\n  const columnIndexes = columnOrder.map(name => asyncColumns.findIndex(column => column.pathInSchema[0] === name))\n\n  // transpose columns into rows\n  const selectCount = selectEnd - selectStart\n  if (rowFormat === 'object') {\n    /** @type {Record<string, any>[]} */\n    const groupData = new Array(selectCount)\n    for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n      const row = selectStart + selectRow\n      // return each row as an object\n      /** @type {Record<string, any>} */\n      const rowData = {}\n      for (let i = 0; i < asyncColumns.length; i++) {\n        rowData[asyncColumns[i].pathInSchema[0]] = columnDatas[i][row]\n      }\n      groupData[selectRow] = rowData\n    }\n    return groupData\n  }\n\n  /** @type {any[][]} */\n  const groupData = new Array(selectCount)\n  for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n    const row = selectStart + selectRow\n    // return each row as an array\n    const rowData = new Array(asyncColumns.length)\n    for (let i = 0; i < columnOrder.length; i++) {\n      if (columnIndexes[i] >= 0) {\n        rowData[i] = columnDatas[columnIndexes[i]][row]\n      }\n    }\n    groupData[selectRow] = rowData\n  }\n  return groupData\n}\n\n/**\n * Assemble physical columns into top-level columns asynchronously.\n *\n * @param {AsyncRowGroup} asyncRowGroup\n * @param {SchemaTree} schemaTree\n * @returns {AsyncRowGroup}\n */\nexport function assembleAsync(asyncRowGroup, schemaTree) {\n  const { asyncColumns } = asyncRowGroup\n  /** @type {AsyncColumn[]} */\n  const assembled = []\n  for (const child of schemaTree.children) {\n    if (child.children.length) {\n      const childColumns = asyncColumns.filter(column => column.pathInSchema[0] === child.element.name)\n      if (!childColumns.length) continue\n\n      // wait for all child columns to be read\n      /** @type {Map<string, DecodedArray>} */\n      const flatData = new Map()\n      const data = Promise.all(childColumns.map(column => {\n        return column.data.then(columnData => {\n          flatData.set(column.pathInSchema.join('.'), flatten(columnData))\n        })\n      })).then(() => {\n        // assemble the column\n        assembleNested(flatData, child)\n        const flatColumn = flatData.get(child.path.join('.'))\n        if (!flatColumn) throw new Error('parquet column data not assembled')\n        return [flatColumn]\n      })\n\n      assembled.push({ pathInSchema: child.path, data })\n    } else {\n      // leaf node, return the column\n      const asyncColumn = asyncColumns.find(column => column.pathInSchema[0] === child.element.name)\n      if (asyncColumn) {\n        assembled.push(asyncColumn)\n      }\n    }\n  }\n  return { ...asyncRowGroup, asyncColumns: assembled }\n}\n","import { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetPlan, prefetchAsyncBuffer } from './plan.js'\nimport { assembleAsync, asyncGroupToRows, readRowGroup } from './rowgroup.js'\nimport { concat, flatten } from './utils.js'\n\n/**\n * @import {AsyncRowGroup, DecodedArray, ParquetReadOptions, BaseParquetReadOptions} from '../src/types.js'\n */\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete.\n * Errors are thrown on the returned promise.\n * Data is returned in callbacks onComplete, onChunk, onPage, NOT the return promise.\n * See parquetReadObjects for a more convenient API.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed, all errors are thrown here\n */\nexport async function parquetRead(options) {\n  // load metadata if not provided\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n\n  // read row groups\n  const asyncGroups = parquetReadAsync(options)\n\n  const { rowStart = 0, rowEnd, columns, onChunk, onComplete, rowFormat } = options\n\n  // skip assembly if no onComplete or onChunk, but wait for reading to finish\n  if (!onComplete && !onChunk) {\n    for (const { asyncColumns } of asyncGroups) {\n      for (const { data } of asyncColumns) await data\n    }\n    return\n  }\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  // onChunk emit all chunks (don't await)\n  if (onChunk) {\n    for (const asyncGroup of assembled) {\n      for (const asyncColumn of asyncGroup.asyncColumns) {\n        asyncColumn.data.then(columnDatas => {\n          let rowStart = asyncGroup.groupStart\n          for (const columnData of columnDatas) {\n            onChunk({\n              columnName: asyncColumn.pathInSchema[0],\n              columnData,\n              rowStart,\n              rowEnd: rowStart + columnData.length,\n            })\n            rowStart += columnData.length\n          }\n        })\n      }\n    }\n  }\n\n  // onComplete transpose column chunks to rows\n  if (onComplete) {\n    // loosen the types to avoid duplicate code\n    /** @type {any[]} */\n    const rows = []\n    for (const asyncGroup of assembled) {\n      // filter to rows in range\n      const selectStart = Math.max(rowStart - asyncGroup.groupStart, 0)\n      const selectEnd = Math.min((rowEnd ?? Infinity) - asyncGroup.groupStart, asyncGroup.groupRows)\n      // transpose column chunks to rows in output\n      const groupData = rowFormat === 'object' ?\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, 'object') :\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, 'array')\n      concat(rows, groupData)\n    }\n    onComplete(rows)\n  } else {\n    // wait for all async groups to finish (complete takes care of this)\n    for (const { asyncColumns } of assembled) {\n      for (const { data } of asyncColumns) await data\n    }\n  }\n}\n\n/**\n * @param {ParquetReadOptions} options read options\n * @returns {AsyncRowGroup[]}\n */\nexport function parquetReadAsync(options) {\n  if (!options.metadata) throw new Error('parquet requires metadata')\n  // TODO: validate options (start, end, columns, etc)\n\n  // prefetch byte ranges\n  const plan = parquetPlan(options)\n  options.file = prefetchAsyncBuffer(options.file, plan)\n\n  // read row groups\n  return plan.groups.map(groupPlan => readRowGroup(options, plan, groupPlan))\n}\n\n/**\n * Reads a single column from a parquet file.\n *\n * @param {BaseParquetReadOptions} options\n * @returns {Promise<DecodedArray>}\n */\nexport async function parquetReadColumn(options) {\n  if (options.columns?.length !== 1) {\n    throw new Error('parquetReadColumn expected columns: [columnName]')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n  const asyncGroups = parquetReadAsync(options)\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  /** @type {DecodedArray[]} */\n  const columnData = []\n  for (const rg of assembled) {\n    columnData.push(flatten(await rg.asyncColumns[0].data))\n  }\n  return flatten(columnData)\n}\n\n/**\n * This is a helper function to read parquet row data as a promise.\n * It is a wrapper around the more configurable parquetRead function.\n *\n * @param {Omit<ParquetReadOptions, 'onComplete'>} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      ...options,\n      rowFormat: 'object', // force object output\n      onComplete,\n    }).catch(reject)\n  })\n}\n","import * as THREE from 'three';\r\nimport { BimGeometry } from './bimGeometry';\r\n\r\nexport function buildGeometryGroup(bim: BimGeometry): THREE.Group \r\n{\r\n  console.time(\"Creating geometry group\");\r\n  const root = new THREE.Group();\r\n\r\n  const vertexCount = bim.VertexX.length;\r\n  const indexCount = bim.IndexBuffer.length;\r\n  const meshCount = bim.MeshVertexOffset.length;\r\n  const matCount = bim.MaterialRed.length;\r\n  const elementCount = bim.ElementMeshIndex.length;\r\n  const transformCount = bim.TransformTX.length;\r\n\r\n  console.log({ vertexCount, indexCount, meshCount, matCount, elementCount, transformCount });\r\n\r\n  const transformMatrices = computeTransforms(bim);\r\n  const meshGeometries = computeMeshGeometries(bim);\r\n  const materials = computeMaterials(bim);\r\n\r\n  // ---------- Group elements by (meshIndex, materialIndex) ----------\r\n  console.time('Grouping elements');\r\n  type Bucket = {\r\n    meshIndex: number;\r\n    materialIndex: number;\r\n    elementIndices: number[];\r\n  };\r\n\r\n  const buckets = new Map<string, Bucket>();\r\n\r\n  for (let ei = 0; ei < elementCount; ei++) {\r\n    const meshIndex = bim.ElementMeshIndex[ei];\r\n    const materialIndex = bim.ElementMaterialIndex[ei];\r\n\r\n    if (meshIndex < 0 || meshIndex >= meshCount) continue;\r\n    if (materialIndex < 0 || materialIndex >= matCount) continue;\r\n    if (!meshGeometries[meshIndex]) continue;\r\n\r\n    const key = `${meshIndex}|${materialIndex}`;\r\n    let bucket = buckets.get(key);\r\n    if (!bucket) {\r\n      bucket = { meshIndex, materialIndex, elementIndices: [] };\r\n      buckets.set(key, bucket);\r\n    }\r\n    bucket.elementIndices.push(ei);\r\n  }\r\n  console.timeEnd('Grouping elements');\r\n\r\n  // ---------- Prepare static merge data (singletons) ----------\r\n  type StaticEntry = {\r\n    geom: THREE.BufferGeometry;\r\n    transformIndex: number;\r\n    entityIndex: number;\r\n    meshIndex: number;\r\n  };\r\n\r\n  // materialIndex -> static entries\r\n  const staticByMaterial = new Map<number, StaticEntry[]>();\r\n\r\n  // ---------- Create instanced meshes and collect static entries ----------\r\n  console.time('Creating instanced meshes');\r\n\r\n  for (const [, bucket] of buckets) {\r\n    const { meshIndex, materialIndex, elementIndices } = bucket;\r\n    const geom = meshGeometries[meshIndex];\r\n    if (!geom) continue;\r\n\r\n    const material = materials[materialIndex];\r\n    const count = elementIndices.length;\r\n\r\n    if (count === 1) {\r\n      const ei = elementIndices[0];\r\n      const ti = bim.ElementTransformIndex[ei];\r\n      const entityIndex = bim.ElementEntityIndex[ei] ?? -1;\r\n\r\n      let list = staticByMaterial.get(materialIndex);\r\n      if (!list) {\r\n        list = [];\r\n        staticByMaterial.set(materialIndex, list);\r\n      }\r\n\r\n      list.push({\r\n        geom,\r\n        transformIndex: ti,\r\n        entityIndex,\r\n        meshIndex,\r\n      });\r\n\r\n      continue;\r\n    }\r\n\r\n    const instanced = new THREE.InstancedMesh(geom, material, count);\r\n    instanced.instanceMatrix.setUsage(THREE.StaticDrawUsage);\r\n\r\n    for (let i = 0; i < count; i++) {\r\n      const ei = elementIndices[i];\r\n      const ti = bim.ElementTransformIndex[ei];\r\n      instanced.setMatrixAt(i, transformMatrices[ti]);\r\n    }\r\n\r\n    (instanced.userData as any).meshIndex = meshIndex;\r\n    (instanced.userData as any).materialIndex = materialIndex;\r\n    instanced.frustumCulled = false;\r\n    instanced.matrixAutoUpdate = false;\r\n    instanced.matrixWorldNeedsUpdate = false;\r\n    root.add(instanced);\r\n  }\r\n\r\n  console.timeEnd('Creating instanced meshes');\r\n\r\n  console.time('Merging static meshes by material');\r\n  const identity = new THREE.Matrix4();\r\n  const useBatchedMesh = false; \r\n\r\n  for (const [materialIndex, entries] of staticByMaterial) \r\n  {\r\n    if (!entries || entries.length === 0) continue;\r\n\r\n    const material = materials[materialIndex];\r\n\r\n    // --- Option A: BatchedMesh (keeps per-entry transforms, one-ish draw)\r\n    if (useBatchedMesh) {\r\n      // BatchedMesh wants a \"prototype\" geometry + material.\r\n      // It then lets you add geometries (sub-meshes) with transforms.\r\n      //\r\n      // NOTE: API differs slightly across revisions; this pattern matches the common one:\r\n      //   const batched = new THREE.BatchedMesh(maxInstances, maxVertices, maxIndices, material)\r\n      //   const id = batched.addGeometry(geom) / add(geom, matrix) ...\r\n      //\r\n      // We'll compute maxima.\r\n      let maxVertices = 0;\r\n      let maxIndices = 0;\r\n\r\n      for (const e of entries) {\r\n        const pos = e.geom.getAttribute('position');\r\n        if (!pos) continue;\r\n        maxVertices += pos.count;\r\n\r\n        const index = e.geom.getIndex();\r\n        maxIndices += index ? index.count : pos.count; // non-indexed fallback\r\n      }\r\n\r\n      // If theres nothing valid, skip\r\n      if (maxVertices === 0) continue;\r\n\r\n      // Create the batched mesh.\r\n      // Signature varies; this is the most common current form:\r\n      const batched = new THREE.BatchedMesh(\r\n        entries.length,      // max instance count (items)\r\n        maxVertices,         // max vertices across all items\r\n        maxIndices,          // max indices across all items\r\n        material\r\n      );\r\n\r\n      batched.name = `BatchedStatic_Material_${materialIndex}`;\r\n      batched.matrixAutoUpdate = false;\r\n\r\n      // Add each geometry with its transform\r\n      for (let i = 0; i < entries.length; i++) {\r\n        const { geom, transformIndex } = entries[i];\r\n        const m = transformMatrices[transformIndex];\r\n        let geometryId = batched.addGeometry(geom, m);\r\n        let instanceId = batched.addInstance(geometryId);\r\n        batched.setMatrixAt(instanceId, m);        \r\n      }\r\n    \r\n    batched.perObjectFrustumCulled = false;\r\n    batched.frustumCulled = false;\r\n    batched.matrixAutoUpdate = false;\r\n    batched.matrixWorldNeedsUpdate = false;\r\n    root.add(batched);\r\n    continue;\r\n  }\r\n\r\n  // --- Option B: MergeGeometries (bakes transforms, cheapest runtime)\r\n  if (entries.length === 1) {\r\n    const { geom, transformIndex } = entries[0];\r\n    const matrix = transformMatrices[transformIndex];\r\n\r\n    const mesh = new THREE.Mesh(geom, material);\r\n    mesh.matrixAutoUpdate = false;\r\n    mesh.matrix.copy(matrix); // set local matrix directly\r\n    root.add(mesh);\r\n    continue;\r\n  }\r\n\r\n  const geomsToMerge: THREE.BufferGeometry[] = [];\r\n\r\n  for (const entry of entries) {\r\n    const { geom, transformIndex } = entry;\r\n    const m = transformMatrices[transformIndex];\r\n\r\n    // IMPORTANT: do NOT mutate the original geometry\r\n    const g = geom.clone();\r\n\r\n    if (!m.equals(identity)) {\r\n      g.applyMatrix4(m);\r\n    }\r\n\r\n    geomsToMerge.push(g);\r\n  }\r\n\r\n  if (geomsToMerge.length === 0) continue;\r\n\r\n  const mergedGeometry = mergeGeometries(geomsToMerge);\r\n  const mergedMesh = new THREE.Mesh(mergedGeometry, material);\r\n  mergedMesh.name = `MergedStatic_Material_${materialIndex}`;\r\n\r\n  root.add(mergedMesh);\r\n\r\n  // Optional: free clones if you dont need them anymore\r\n  for (const g of geomsToMerge) g.dispose?.();\r\n}\r\n\r\n  console.timeEnd('Merging static meshes by material');\r\n\r\n  // Convert Z-Up to Y-Up \r\n  root.rotation.x = -Math.PI / 2;\r\n\r\n  console.timeEnd(\"Creating geometry group\");\r\n  return root;\r\n}\r\n\r\nexport function computeTransforms(bim: BimGeometry)\r\n{\r\n  const {\r\n    TransformTX,\r\n    TransformTY,\r\n    TransformTZ,\r\n    TransformQX,\r\n    TransformQY,\r\n    TransformQZ,\r\n    TransformQW,\r\n    TransformSX,\r\n    TransformSY,\r\n    TransformSZ,\r\n  } = bim;\r\n\r\n  const tmpPos = new THREE.Vector3();\r\n  const tmpQuat = new THREE.Quaternion();\r\n  const tmpScale = new THREE.Vector3();\r\n  const transformCount = TransformTX.length;\r\n    \r\n  const matrices = new Array(transformCount);\r\n  \r\n  for (let ti = 0; ti < transformCount; ti++) \r\n  {\r\n    const tx = TransformTX[ti];\r\n    const ty = TransformTY[ti];\r\n    const tz = TransformTZ[ti];\r\n    const sx = TransformSX[ti];\r\n    const sy = TransformSY[ti];\r\n    const sz = TransformSZ[ti];\r\n    const qx = TransformQX[ti];\r\n    const qy = TransformQY[ti];\r\n    const qz = TransformQZ[ti];\r\n    const qw = TransformQW[ti];\r\n\r\n    const m = new THREE.Matrix4();\r\n    tmpPos.set(tx, ty, tz);\r\n    tmpQuat.set(qx, qy, qz, qw);\r\n    tmpScale.set(sx, sy, sz);\r\n    m.compose(tmpPos, tmpQuat, tmpScale);\r\n  \r\n    matrices[ti] = m;\r\n  }\r\n  return matrices;\r\n}\r\n\r\nexport function mergeGeometries(\r\n  geometries: Array<THREE.BufferGeometry>\r\n): THREE.BufferGeometry \r\n{\r\n  // First pass: count total vertices and indices\r\n  let indexCount = 0;\r\n  let posCount = 0;\r\n\r\n  for (let i = 0, l = geometries.length; i < l; i++) {\r\n    const geometry = geometries[i];\r\n\r\n    const index = geometry.getIndex();\r\n    const position = geometry.getAttribute('position');\r\n\r\n    if (!index) throw new Error('mergeGeometries: geometry has no index buffer');\r\n    if (!position) throw new Error('mergeGeometries: geometry has no position attribute');\r\n\r\n    indexCount += index.count;\r\n    posCount += position.count;\r\n  }\r\n\r\n  // Allocate merged buffers\r\n  // Assuming positions are vec3 (itemSize = 3)\r\n  const mergedPositions = new Float32Array(posCount * 3);\r\n  const mergedIndices = new Uint32Array(indexCount);\r\n\r\n  let indexOffset = 0;      // how many indices we've already written\r\n  let vertexOffset = 0;     // how many vertices we've already written\r\n\r\n  // Second pass: copy data\r\n  for (let i = 0, l = geometries.length; i < l; i++) {\r\n    const geometry = geometries[i];\r\n\r\n    const posAttr = geometry.getAttribute('position') as THREE.BufferAttribute;\r\n    const indexAttr = geometry.getIndex() as THREE.BufferAttribute;\r\n\r\n    const srcPosArray = posAttr.array as Float32Array;\r\n    const srcIndexArray = indexAttr.array as Int32Array;\r\n\r\n    const vertCount = posAttr.count;\r\n    const idxCount = indexAttr.count;\r\n\r\n    const posItemSize = posAttr.itemSize; // usually 3\r\n\r\n    // --- Copy positions ---\r\n    // We only copy the used segment (0..vertCount*itemSize)\r\n    const srcPosLength = vertCount * posItemSize;\r\n    const dstPosOffset = vertexOffset * posItemSize;\r\n    mergedPositions.set(srcPosArray.subarray(0, srcPosLength), dstPosOffset);\r\n\r\n    // --- Copy indices, with vertex offset ---\r\n    for (let j = 0; j < idxCount; j++) {\r\n      mergedIndices[indexOffset + j] = srcIndexArray[j] + vertexOffset;\r\n    }\r\n\r\n    vertexOffset += vertCount;\r\n    indexOffset += idxCount;\r\n  }\r\n\r\n  // Build merged geometry\r\n  const mergedGeom = new THREE.BufferGeometry();\r\n  mergedGeom.setAttribute('position', new THREE.BufferAttribute(mergedPositions, 3));\r\n  mergedGeom.setIndex(new THREE.BufferAttribute(mergedIndices, 1));\r\n  return mergedGeom;\r\n}\r\n\r\nfunction computeMeshGeometries(bim: BimGeometry): Array<THREE.BufferGeometry>\r\n{\r\n  const meshCount = bim.MeshVertexOffset.length;\r\n  const indexCount = bim.IndexBuffer.length;\r\n  const vertexCount = bim.VertexX.length;\r\n  const meshGeometries: Array<THREE.BufferGeometry> = new Array(meshCount);\r\n\r\n  const {\r\n    VertexX,\r\n    VertexY,\r\n    VertexZ,\r\n    IndexBuffer,\r\n    MeshVertexOffset,\r\n    MeshIndexOffset,\r\n  } = bim;\r\n\r\n  for (let mi = 0; mi < meshCount; mi++) \r\n  {\r\n    const iStart = MeshIndexOffset[mi];\r\n    const iEnd = mi + 1 < meshCount ? MeshIndexOffset[mi + 1] : indexCount;\r\n    const iCount = iEnd - iStart;\r\n\r\n    const vStart = MeshVertexOffset[mi];\r\n    const vEnd = mi + 1 < meshCount ? MeshVertexOffset[mi + 1] : vertexCount;\r\n    const vCount = vEnd - vStart;\r\n\r\n    if (iCount === 0 || vCount === 0) continue;\r\n\r\n    const indexArray = IndexBuffer.subarray(iStart, iEnd);\r\n\r\n    const vertexMultiplier = 10_000.0;\r\n    const positionArray = new Float32Array(vCount * 3);\r\n    for (let vi = 0; vi < vCount; vi++) {\r\n      positionArray[vi*3+0] = VertexX[vi + vStart] / vertexMultiplier;\r\n      positionArray[vi*3+1] = VertexY[vi + vStart] / vertexMultiplier;\r\n      positionArray[vi*3+2] = VertexZ[vi + vStart] / vertexMultiplier;\r\n    }\r\n\r\n    const geom = new THREE.BufferGeometry();\r\n    geom.setAttribute('position', new THREE.BufferAttribute(positionArray, 3));\r\n    geom.setIndex(new THREE.BufferAttribute(indexArray, 1));\r\n    meshGeometries[mi] = geom;\r\n  }\r\n\r\n  return meshGeometries;\r\n}\r\n\r\nfunction computeMaterials(bim: BimGeometry): Array<THREE.MeshStandardMaterial>\r\n{\r\n  const numMaterials = bim.MaterialAlpha.length;\r\n  const materials = new Array(numMaterials);\r\n\r\n  for (let mi=0; mi < numMaterials; mi++)\r\n  {\r\n    const r = bim.MaterialRed[mi] / 255;\r\n    const g = bim.MaterialGreen[mi] / 255;\r\n    const b = bim.MaterialBlue[mi] / 255;\r\n    const a = bim.MaterialAlpha[mi]  / 255;\r\n    const roughness = bim.MaterialRoughness[mi] / 255;\r\n    const metalness = bim.MaterialMetallic[mi] / 255;\r\n\r\n    const mat = new THREE.MeshStandardMaterial({\r\n      color: new THREE.Color(r, g, b),\r\n      opacity: a,\r\n      flatShading: true,\r\n      transparent: a < 0.999,\r\n      roughness,\r\n      metalness,\r\n      side: THREE.DoubleSide,\r\n    });\r\n\r\n    materials[mi] = mat;\r\n  }\r\n  return materials;\r\n}","import * as THREE from 'three';\r\nimport JSZip from 'jszip';\r\nimport { parquetRead, parquetMetadataAsync, parquetSchema, ColumnData, parquetReadObjects, ParquetReadOptions } from 'hyparquet';\r\nimport { compressors } from 'hyparquet-compressors';\r\nimport { BimGeometry } from './bimGeometry';\r\nimport { buildGeometryGroup } from './buildGeometryGroup';\r\n\r\n/**\r\n * Loader that takes a URL to a .ZIP or .BOS file containing BIM Open Schema geometry parquet tables:\r\n */\r\nexport class BimOpenSchemaLoader \r\n{\r\n  async load(source: string): Promise<THREE.Group> {\r\n    const response = await fetch(source);\r\n    if (!response.ok) {\r\n      throw new Error(`Failed to fetch BOS from ${source}: ${response.status} ${response.statusText}`);\r\n    }\r\n\r\n    const arrayBuffer = await response.arrayBuffer();\r\n    const zip = await JSZip.loadAsync(arrayBuffer);\r\n    const bim = await loadBimGeometryFromZip(zip);\r\n    return buildGeometryGroup(bim);\r\n  }\r\n}\r\n\r\n/**\r\n * Reads the BOS parquet tables from a JSZip archive into a BimGeometry object.\r\n * This is the same idea as the previous browser version, just using package imports.\r\n */\r\nexport async function loadBimGeometryFromZip(zip: JSZip): Promise<BimGeometry> \r\n{\r\n  // Find the file in the zip archive\r\n  function findFileEndingWith(suffix: string): string {\r\n    const lowerSuffix = suffix.toLowerCase();\r\n    const name = Object.keys(zip.files).find((n) =>\r\n      n.toLowerCase().endsWith(lowerSuffix));\r\n    if (!name) \r\n      throw new Error(`Could not find \"${suffix}\" in zip archive.`);\r\n    return name;\r\n  }\r\n\r\n  // Read the table, and put the columns directly\r\n  async function readParquetTable(name: string, bimObject: any, ctor: any) {\r\n    const entryName = findFileEndingWith(name);\r\n    const file = await zip.files[entryName].async('arraybuffer');\r\n    const metadata = await parquetMetadataAsync(file);\r\n    await parquetRead({file, compressors, metadata, onChunk(chunk: ColumnData) {\r\n      let data = chunk.columnData;\r\n      if (data.constructor.name != ctor.name)\r\n      {\r\n        // Some arrays are typed, and some aren't. Don't ask me why?! \r\n        data = new ctor(data);         \r\n      }\r\n      bimObject[chunk.columnName] = ctor ? new ctor(data) : ctor;\r\n    }});\r\n  }\r\n\r\n  console.time(\"Reading parquet tables\");\r\n  const bim = {}\r\n  await readParquetTable('Instances.parquet', bim, Int32Array);\r\n  await readParquetTable('VertexBuffer.parquet', bim, Int32Array);\r\n  await readParquetTable('IndexBuffer.parquet', bim, Uint32Array);\r\n  await readParquetTable('Meshes.parquet', bim, Int32Array);\r\n  await readParquetTable('Materials.parquet', bim, Uint8Array);\r\n  await readParquetTable('Transforms.parquet', bim, Float32Array);  \r\n  console.timeEnd(\"Reading parquet tables\");\r\n  return bim as BimGeometry;\r\n}\r\n\r\n\r\n"],"names":["decoder","child","schemaTree","logicalType","bitWidth","len","compressors","reader","groupData","rowStart","THREE.Group","THREE.InstancedMesh","THREE.StaticDrawUsage","THREE.Matrix4","THREE.Mesh","THREE.Vector3","THREE.Quaternion","THREE.BufferGeometry","THREE.BufferAttribute","THREE.MeshStandardMaterial","THREE.Color","THREE.DoubleSide"],"mappings":";AAEO,MAAM,eAAe;AAAA,EAC1B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,MAAM,YAAY;AAAA,EACvB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,MAAM,uBAAuB;AAAA,EAClC;AAAA,EACA;AAAA,EACA;AACF;AAGO,MAAM,iBAAiB;AAAA,EAC5B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,MAAM,oBAAoB;AAAA,EAC/B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,MAAM,YAAY;AAAA,EACvB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAUO,MAAM,8BAA8B;AAAA,EACzC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;ACvFO,SAAS,aAAa,QAAQ;AACnC,QAAM,QAAQ,SAAS,MAAM;AAE7B,MAAI,MAAM,SAAS,GAAG;AACpB,WAAO,EAAE,MAAM,SAAS,aAAa,aAAa,QAAQ,KAAK,EAAG;AAAA,EACtE,WAAa,MAAM,SAAS,GAAG;AAC3B,WAAO,EAAE,MAAM,cAAc,aAAa,SAAS,QAAQ,KAAK,EAAG;AAAA,EACvE,WAAa,MAAM,SAAS,GAAG;AAC3B,WAAO,EAAE,MAAM,WAAW,aAAa,YAAY,QAAQ,KAAK,EAAG;AAAA,EACvE,WAAa,MAAM,SAAS,GAAG;AAC3B,UAAM,SAAS,CAAE;AACjB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,aAAO,KAAK,aAAa,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IACnD;AACD,WAAO,EAAE,MAAM,cAAc,aAAa,OAAQ;AAAA,EACtD,WAAa,MAAM,SAAS,GAAG;AAC3B,UAAM,QAAQ,CAAE;AAChB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,YAAM,KAAK,SAAS,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IAC9C;AACD,WAAO,EAAE,MAAM,mBAAmB,aAAa,MAAO;AAAA,EAC1D,WAAa,MAAM,SAAS,GAAG;AAC3B,UAAM,WAAW,CAAE;AACnB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,eAAS,KAAK,YAAY,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IACpD;AACD,WAAO,EAAE,MAAM,gBAAgB,aAAa,SAAU;AAAA,EAC1D,WAAa,MAAM,SAAS,GAAG;AAC3B,UAAM,aAAa,CAAE;AACrB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,iBAAW,KAAK,aAAa,MAAM,CAAC;AAAA,IACrC;AACD,WAAO,EAAE,MAAM,sBAAsB,WAAY;AAAA,EACrD,OAAS;AACL,UAAM,IAAI,MAAM,8BAA8B,MAAM,MAAM;AAAA,EAC3D;AACH;AAgBA,SAAS,SAAS,QAAQ;AACxB,QAAM,EAAE,KAAI,IAAK;AACjB,QAAM,eAAe,KAAK,SAAS,OAAO,QAAQ,MAAM;AACxD,QAAM,UAAU,KAAK,UAAU,OAAO,QAAQ,YAAY;AAC1D,SAAO,UAAU;AAEjB,QAAM,OAAO,UAAU;AACvB,QAAM,QAAQ,KAAK,MAAM,UAAU,GAAI;AAEvC,MAAI,QAAQ;AACZ,MAAI,OAAO,KAAK,QAAQ,GAAG;AACzB,YAAQ,KAAK,UAAU,OAAO,QAAQ,YAAY;AAClD,WAAO,UAAU;AAAA,EAClB;AAGD,MAAI,MAAM;AACV,MAAI;AAAO;AACX,MAAI,UAAU;AAAG;AAEjB,SAAO,EAAE,cAAc,MAAM,KAAK,MAAO;AAC3C;AAOA,SAAS,aAAa,QAAQ,OAAO;AACnC,QAAM,SAAS,CAAE;AACjB,WAAS,IAAI,GAAG,IAAI,MAAM,KAAK,KAAK;AAClC,UAAM,QAAQ,OAAO,KAAK,WAAW,OAAO,QAAQ,MAAM,YAAY;AACtE,WAAO,UAAU;AACjB,WAAO,KAAK,KAAK;AAAA,EAClB;AACD,SAAO;AACT;AAOA,SAAS,SAAS,QAAQ,OAAO;AAC/B,QAAM,SAAS,CAAE;AACjB,WAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,WAAO,KAAK,aAAa,QAAQ,KAAK,CAAC;AAAA,EACxC;AACD,SAAO;AACT;AAOA,SAAS,YAAY,QAAQ,OAAO;AAClC,QAAM,EAAE,KAAI,IAAK;AACjB,QAAM,QAAQ,CAAE;AAChB,WAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,UAAM,QAAQ,KAAK,UAAU,OAAO,QAAQ,MAAM,YAAY;AAC9D,WAAO,UAAU;AACjB,UAAM,KAAK,SAAS,QAAQ,EAAE,GAAG,OAAO,MAAK,CAAE,CAAC;AAAA,EACjD;AACD,SAAO;AACT;ACtHA,MAAMA,YAAU,IAAI,YAAa;AAM1B,MAAM,kBAAkB;AAAA,EAC7B,0BAA0B,QAAQ;AAChC,WAAO,IAAI,KAAK,OAAO,MAAM,CAAC;AAAA,EAC/B;AAAA,EACD,0BAA0B,QAAQ;AAChC,WAAO,IAAI,KAAK,OAAO,SAAS,KAAK,CAAC;AAAA,EACvC;AAAA,EACD,yBAAyB,OAAO;AAC9B,WAAO,IAAI,KAAK,OAAO,QAAQ,QAAQ,CAAC;AAAA,EACzC;AAAA,EACD,aAAa,MAAM;AACjB,WAAO,IAAI,KAAK,OAAO,KAAQ;AAAA,EAChC;AAAA,EACD,gBAAgB,OAAO;AACrB,WAAO,SAASA,UAAQ,OAAO,KAAK;AAAA,EACrC;AAAA,EACD,kBAAkB,OAAO;AACvB,WAAO,SAAS,aAAa,EAAE,MAAM,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU,GAAG,QAAQ,EAAC,CAAE;AAAA,EACjH;AAAA,EACD,mBAAmB,OAAO;AACxB,WAAO,SAAS,aAAa,EAAE,MAAM,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU,GAAG,QAAQ,EAAC,CAAE;AAAA,EACjH;AACH;AAWO,SAAS,sBAAsB,MAAM,YAAY,UAAU,eAAe;AAC/E,MAAI,cAAc,SAAS,SAAS,aAAa,GAAG;AAClD,QAAI,SAAS;AACb,QAAI,gBAAgB,cAAc,EAAE,sBAAsB,aAAa;AAErE,eAAS,IAAI,WAAW,YAAY,KAAK,MAAM;AAAA,IAChD;AACD,aAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,KAAK;AACpC,aAAO,KAAK,WAAW,KAAK;AAAA,IAC7B;AACD,WAAO;AAAA,EACX,OAAS;AACL,WAAO,QAAQ,MAAM,aAAa;AAAA,EACnC;AACH;AASO,SAAS,QAAQ,MAAM,eAAe;AAC3C,QAAM,EAAE,SAAS,SAAS,OAAO,KAAM,IAAG;AAC1C,QAAM,EAAE,MAAM,gBAAgB,OAAO,cAAc,MAAK,IAAK;AAC7D,MAAI,UAAU,WAAW;AACvB,UAAM,QAAQ,QAAQ,SAAS;AAC/B,UAAM,SAAS,MAAM,CAAC;AACtB,UAAM,MAAM,IAAI,MAAM,KAAK,MAAM;AACjC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,UAAI,KAAK,cAAc,YAAY;AACjC,YAAI,KAAK,aAAa,KAAK,EAAE,IAAI;AAAA,MACzC,OAAa;AACL,YAAI,KAAK,OAAO,KAAK,EAAE,IAAI;AAAA,MAC5B;AAAA,IACF;AACD,WAAO;AAAA,EACR;AACD,MAAI,CAAC,SAAS,SAAS,SAAS;AAC9B,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,yBAAyB,gBAAgB,CAAC,CAAC,CAAC;AAAA,EACtF;AACD,MAAI,UAAU,QAAQ;AACpB,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,aAAa,CAAC,CAAC;AAAA,EACzD;AACD,MAAI,UAAU,oBAAoB;AAChC,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,0BAA0B,CAAC,CAAC;AAAA,EACtE;AACD,MAAI,UAAU,oBAAoB;AAChC,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,0BAA0B,CAAC,CAAC;AAAA,EACtE;AACD,MAAI,UAAU,QAAQ;AACpB,WAAO,KAAK,IAAI,OAAK,KAAK,MAAMA,UAAQ,OAAO,CAAC,CAAC,CAAC;AAAA,EACnD;AACD,MAAI,UAAU,QAAQ;AACpB,UAAM,IAAI,MAAM,4BAA4B;AAAA,EAC7C;AACD,MAAI,UAAU,YAAY;AACxB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EACjD;AACD,MAAI,OAAO,SAAS,YAAY;AAC9B,WAAO,KAAK,IAAI,OAAK,QAAQ,kBAAkB,CAAC,CAAC;AAAA,EAClD;AACD,MAAI,OAAO,SAAS,aAAa;AAC/B,WAAO,KAAK,IAAI,OAAK,QAAQ,mBAAmB,CAAC,CAAC;AAAA,EACnD;AACD,MAAI,UAAU,UAAU,OAAO,SAAS,YAAY,QAAQ,SAAS,cAAc;AACjF,WAAO,KAAK,IAAI,OAAK,QAAQ,gBAAgB,CAAC,CAAC;AAAA,EAChD;AACD,MAAI,UAAU,aAAa,OAAO,SAAS,aAAa,MAAM,aAAa,MAAM,CAAC,MAAM,UAAU;AAChG,QAAI,gBAAgB,eAAe;AACjC,aAAO,IAAI,eAAe,KAAK,QAAQ,KAAK,YAAY,KAAK,MAAM;AAAA,IACpE;AACD,UAAM,MAAM,IAAI,eAAe,KAAK,MAAM;AAC1C,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ;AAAK,UAAI,KAAK,OAAO,KAAK,EAAE;AAC5D,WAAO;AAAA,EACR;AACD,MAAI,UAAU,aAAa,OAAO,SAAS,aAAa,MAAM,aAAa,MAAM,CAAC,MAAM,UAAU;AAChG,QAAI,gBAAgB,YAAY;AAC9B,aAAO,IAAI,YAAY,KAAK,QAAQ,KAAK,YAAY,KAAK,MAAM;AAAA,IACjE;AACD,UAAM,MAAM,IAAI,YAAY,KAAK,MAAM;AACvC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ;AAAK,UAAI,KAAK,KAAK;AACnD,WAAO;AAAA,EACR;AACD,MAAI,OAAO,SAAS,WAAW;AAC7B,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,YAAY;AAAA,EACzC;AACD,MAAI,OAAO,SAAS,aAAa;AAC/B,UAAM,EAAE,KAAI,IAAK;AAEjB,QAAI,SAAS,QAAQ;AACrB,QAAI,SAAS;AAAU,eAAS,QAAQ;AACxC,QAAI,SAAS;AAAS,eAAS,QAAQ;AACvC,UAAM,MAAM,IAAI,MAAM,KAAK,MAAM;AACjC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,UAAI,KAAK,OAAO,KAAK,EAAE;AAAA,IACxB;AACD,WAAO;AAAA,EACR;AACD,SAAO;AACT;AAMO,SAAS,aAAa,OAAO;AAClC,MAAI,CAAC,MAAM;AAAQ,WAAO;AAE1B,MAAI,QAAQ;AACZ,aAAW,QAAQ,OAAO;AACxB,YAAQ,QAAQ,OAAO,OAAO,IAAI;AAAA,EACnC;AAGD,QAAM,OAAO,MAAM,SAAS;AAC5B,MAAI,SAAS,MAAM,OAAO,OAAO,CAAC,GAAG;AACnC,aAAS,MAAM,OAAO,IAAI;AAAA,EAC3B;AAED,SAAO,OAAO,KAAK;AACrB;AAOA,SAAS,gBAAgB,OAAO;AAC9B,QAAM,QAAQ,SAAS,OAAO;AAC9B,QAAM,OAAO,QAAQ;AACrB,SAAO,OAAO,kBAAkB;AAClC;AAMO,SAAS,aAAa,OAAO;AAClC,MAAI,CAAC;AAAO,WAAO;AACnB,QAAM,QAAQ,MAAM,MAAM,IAAI,MAAM;AACpC,QAAM,OAAO,SAAS,KAAK,KAAK;AAChC,QAAM,MAAM,SAAS,KAAK;AAC1B,QAAM,OAAO,QAAQ;AACrB,MAAI,QAAQ;AAAG,WAAO,OAAO,KAAK,OAAO,OAAO;AAChD,MAAI,QAAQ;AAAM,WAAO,OAAO,MAAM,OAAO;AAC7C,SAAO,OAAO,MAAM,MAAM,OAAO,IAAI,OAAO;AAC9C;ACxLA,SAAS,WAAW,QAAQ,WAAW,MAAM;AAC3C,QAAM,UAAU,OAAO;AACvB,QAAM,WAAW,CAAE;AACnB,MAAI,QAAQ;AAGZ,MAAI,QAAQ,cAAc;AACxB,WAAO,SAAS,SAAS,QAAQ,cAAc;AAC7C,YAAM,eAAe,OAAO,YAAY;AACxC,YAAM,QAAQ,WAAW,QAAQ,YAAY,OAAO,CAAC,GAAG,MAAM,aAAa,IAAI,CAAC;AAChF,eAAS,MAAM;AACf,eAAS,KAAK,KAAK;AAAA,IACpB;AAAA,EACF;AAED,SAAO,EAAE,OAAO,SAAS,UAAU,KAAM;AAC3C;AASO,SAAS,cAAc,QAAQ,MAAM;AAC1C,MAAI,OAAO,WAAW,QAAQ,GAAG,CAAA,CAAE;AACnC,QAAM,OAAO,CAAC,IAAI;AAClB,aAAW,QAAQ,MAAM;AACvB,UAAM,QAAQ,KAAK,SAAS,KAAK,CAAAC,WAASA,OAAM,QAAQ,SAAS,IAAI;AACrE,QAAI,CAAC;AAAO,YAAM,IAAI,MAAM,qCAAqC,MAAM;AACvE,SAAK,KAAK,KAAK;AACf,WAAO;AAAA,EACR;AACD,SAAO;AACT;AAQO,SAAS,mBAAmBC,aAAY;AAE7C,QAAM,UAAU,CAAE;AAElB,WAAS,SAAS,MAAM;AACtB,QAAI,KAAK,SAAS,QAAQ;AACxB,iBAAW,SAAS,KAAK,UAAU;AACjC,iBAAS,KAAK;AAAA,MACf;AAAA,IACP,OAAW;AACL,cAAQ,KAAK,KAAK,QAAQ,IAAI;AAAA,IAC/B;AAAA,EACF;AACD,WAASA,WAAU;AACnB,SAAO;AACT;AAQO,SAAS,sBAAsB,YAAY;AAChD,MAAI,WAAW;AACf,aAAW,EAAE,QAAS,KAAI,YAAY;AACpC,QAAI,QAAQ,oBAAoB,YAAY;AAC1C;AAAA,IACD;AAAA,EACF;AACD,SAAO;AACT;AAQO,SAAS,sBAAsB,YAAY;AAChD,MAAI,WAAW;AACf,aAAW,EAAE,QAAS,KAAI,WAAW,MAAM,CAAC,GAAG;AAC7C,QAAI,QAAQ,oBAAoB,YAAY;AAC1C;AAAA,IACD;AAAA,EACF;AACD,SAAO;AACT;AAQO,SAAS,WAAW,QAAQ;AACjC,MAAI,CAAC;AAAQ,WAAO;AACpB,MAAI,OAAO,QAAQ,mBAAmB;AAAQ,WAAO;AACrD,MAAI,OAAO,SAAS,SAAS;AAAG,WAAO;AAEvC,QAAM,aAAa,OAAO,SAAS;AACnC,MAAI,WAAW,SAAS,SAAS;AAAG,WAAO;AAC3C,MAAI,WAAW,QAAQ,oBAAoB;AAAY,WAAO;AAE9D,SAAO;AACT;AAQO,SAAS,UAAU,QAAQ;AAChC,MAAI,CAAC;AAAQ,WAAO;AACpB,MAAI,OAAO,QAAQ,mBAAmB;AAAO,WAAO;AACpD,MAAI,OAAO,SAAS,SAAS;AAAG,WAAO;AAEvC,QAAM,aAAa,OAAO,SAAS;AACnC,MAAI,WAAW,SAAS,WAAW;AAAG,WAAO;AAC7C,MAAI,WAAW,QAAQ,oBAAoB;AAAY,WAAO;AAE9D,QAAM,WAAW,WAAW,SAAS,KAAK,WAAS,MAAM,QAAQ,SAAS,KAAK;AAC/E,MAAI,UAAU,QAAQ,oBAAoB;AAAY,WAAO;AAE7D,QAAM,aAAa,WAAW,SAAS,KAAK,WAAS,MAAM,QAAQ,SAAS,OAAO;AACnF,MAAI,YAAY,QAAQ,oBAAoB;AAAY,WAAO;AAE/D,SAAO;AACT;AAQO,SAAS,aAAa,YAAY;AACvC,MAAI,WAAW,WAAW;AAAG,WAAO;AACpC,QAAM,CAAA,EAAG,MAAM,IAAI;AACnB,MAAI,OAAO,QAAQ,oBAAoB;AAAY,WAAO;AAC1D,MAAI,OAAO,SAAS;AAAQ,WAAO;AACnC,SAAO;AACT;AC1JO,MAAM,cAAc;AAAA,EACzB,MAAM;AAAA,EACN,MAAM;AAAA,EACN,OAAO;AAAA,EACP,MAAM;AAAA,EACN,KAAK;AAAA,EACL,KAAK;AAAA,EACL,KAAK;AAAA,EACL,QAAQ;AAAA,EACR,QAAQ;AAAA,EACR,MAAM;AAAA,EACN,KAAK;AAAA,EACL,KAAK;AAAA,EACL,QAAQ;AAAA,EACR,MAAM;AACR;AAQO,SAAS,4BAA4B,QAAQ;AAClD,MAAI,UAAU;AAEd,QAAM,QAAQ,CAAE;AAEhB,SAAO,OAAO,SAAS,OAAO,KAAK,YAAY;AAE7C,UAAM,CAAC,MAAM,KAAK,UAAU,IAAI,eAAe,QAAQ,OAAO;AAC9D,cAAU;AAEV,QAAI,SAAS,YAAY,MAAM;AAC7B;AAAA,IACD;AAGD,UAAM,SAAS,SAAS,YAAY,QAAQ,IAAI;AAAA,EACjD;AAED,SAAO;AACT;AAUA,SAAS,YAAY,QAAQ,MAAM;AACjC,UAAQ,MAAI;AAAA,IACZ,KAAK,YAAY;AACf,aAAO;AAAA,IACT,KAAK,YAAY;AACf,aAAO;AAAA,IACT,KAAK,YAAY;AAEf,aAAO,OAAO,KAAK,QAAQ,OAAO,QAAQ;AAAA,IAC5C,KAAK,YAAY;AAAA,IACjB,KAAK,YAAY;AACf,aAAO,WAAW,MAAM;AAAA,IAC1B,KAAK,YAAY;AACf,aAAO,iBAAiB,MAAM;AAAA,IAChC,KAAK,YAAY,QAAQ;AACvB,YAAM,QAAQ,OAAO,KAAK,WAAW,OAAO,QAAQ,IAAI;AACxD,aAAO,UAAU;AACjB,aAAO;AAAA,IACR;AAAA,IACD,KAAK,YAAY,QAAQ;AACvB,YAAM,eAAe,WAAW,MAAM;AACtC,YAAM,WAAW,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,YAAY;AACxG,aAAO,UAAU;AACjB,aAAO;AAAA,IACR;AAAA,IACD,KAAK,YAAY,MAAM;AACrB,YAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,YAAM,WAAW,OAAO;AACxB,UAAI,WAAW,QAAQ;AACvB,UAAI,aAAa,IAAI;AACnB,mBAAW,WAAW,MAAM;AAAA,MAC7B;AACD,YAAM,WAAW,aAAa,YAAY,QAAQ,aAAa,YAAY;AAC3E,YAAM,SAAS,IAAI,MAAM,QAAQ;AACjC,eAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,eAAO,KAAK,WAAW,YAAY,QAAQ,YAAY,IAAI,MAAM,IAAI,YAAY,QAAQ,QAAQ;AAAA,MAClG;AACD,aAAO;AAAA,IACR;AAAA,IACD,KAAK,YAAY,QAAQ;AAEvB,YAAM,eAAe,CAAE;AACvB,UAAI,UAAU;AACd,aAAO,MAAM;AACX,cAAM,CAAC,WAAW,KAAK,UAAU,IAAI,eAAe,QAAQ,OAAO;AACnE,kBAAU;AACV,YAAI,cAAc,YAAY,MAAM;AAClC;AAAA,QACD;AACD,qBAAa,SAAS,SAAS,YAAY,QAAQ,SAAS;AAAA,MAC7D;AACD,aAAO;AAAA,IACR;AAAA,IAED;AACE,YAAM,IAAI,MAAM,0BAA0B,MAAM;AAAA,EACjD;AACH;AASO,SAAS,WAAW,QAAQ;AACjC,MAAI,SAAS;AACb,MAAI,QAAQ;AACZ,SAAO,MAAM;AACX,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,eAAW,OAAO,QAAS;AAC3B,QAAI,EAAE,OAAO,MAAO;AAClB,aAAO;AAAA,IACR;AACD,aAAS;AAAA,EACV;AACH;AAQA,SAAS,cAAc,QAAQ;AAC7B,MAAI,SAAS;AACb,MAAI,QAAQ;AACZ,SAAO,MAAM;AACX,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,cAAU,OAAO,OAAO,GAAI,KAAK;AACjC,QAAI,EAAE,OAAO,MAAO;AAClB,aAAO;AAAA,IACR;AACD,aAAS;AAAA,EACV;AACH;AASO,SAAS,WAAW,QAAQ;AACjC,QAAM,SAAS,WAAW,MAAM;AAEhC,SAAO,WAAW,IAAI,EAAE,SAAS;AACnC;AASO,SAAS,iBAAiB,QAAQ;AACvC,QAAM,SAAS,cAAc,MAAM;AAEnC,SAAO,UAAU,KAAK,EAAE,SAAS;AACnC;AASA,SAAS,eAAe,QAAQ,SAAS;AACvC,QAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,QAAM,OAAO,OAAO;AACpB,MAAI,SAAS,YAAY,MAAM;AAE7B,WAAO,CAAC,GAAG,GAAG,OAAO;AAAA,EACtB;AACD,QAAM,QAAQ,QAAQ;AACtB,QAAM,MAAM,QAAQ,UAAU,QAAQ,WAAW,MAAM;AACvD,SAAO,CAAC,MAAM,KAAK,GAAG;AACxB;AC5LO,SAAS,eAAe,QAAQ,oBAAoB;AAGzD,QAAM,UAAU,oBAAI,IAAK;AACzB,QAAM,MAAM,oBAAoB,KAAK,CAAC,EAAE,UAAU,QAAQ,KAAK,GAAG;AAClE,QAAM,kBAAkB,OAAO,KAAK,MAAM,GAAG,GAAG,YAAY,CAAE;AAC9D,aAAW,CAAC,MAAM,MAAM,KAAK,OAAO,QAAQ,cAAc,GAAG;AAC3D,QAAI,OAAO,aAAa,OAAO;AAC7B;AAAA,IACD;AACD,UAAM,OAAO,OAAO,UAAU,cAAc,cAAc;AAC1D,UAAM,KAAK,OAAO,KAAK,MAAM,OAAO,KAAK,MAAM;AAC/C,UAAM,MAAM,KAAK,GAAG,GAAG,aAAa,GAAG,KAAK,SAAU,MAAK;AAE3D,YAAQ,IAAI,MAAM,EAAE,MAAM,IAAG,CAAE;AAAA,EAChC;AAID,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,UAAM,UAAU,OAAO;AACvB,UAAM,EAAE,cAAc,MAAM,cAAc,iBAAiB,KAAI,IAAK;AACpE,QAAI,cAAc;AAChB,WAAK;AACL;AAAA,IACD;AACD,QAAI,SAAS,gBAAgB,iBAAiB,UAAa,oBAAoB,YAAY;AACzF,cAAQ,eAAe,QAAQ,IAAI,IAAI;AAAA,IACxC;AAAA,EACF;AACH;AC9BO,MAAM,0BAA0B,KAAK;AAE5C,MAAM,UAAU,IAAI,YAAa;AACjC,SAAS,OAAiC,OAAO;AAC/C,SAAO,SAAS,QAAQ,OAAO,KAAK;AACtC;AA0BO,eAAe,qBAAqB,aAAa,EAAE,SAAS,mBAAmB,yBAAyB,aAAa,KAAM,IAAG,IAAI;AACvI,MAAI,CAAC,eAAe,EAAE,YAAY,cAAc;AAAI,UAAM,IAAI,MAAM,8BAA8B;AAGlG,QAAM,eAAe,KAAK,IAAI,GAAG,YAAY,aAAa,gBAAgB;AAC1E,QAAM,eAAe,MAAM,YAAY,MAAM,cAAc,YAAY,UAAU;AAGjF,QAAM,aAAa,IAAI,SAAS,YAAY;AAC5C,MAAI,WAAW,UAAU,aAAa,aAAa,GAAG,IAAI,MAAM,WAAY;AAC1E,UAAM,IAAI,MAAM,uCAAuC;AAAA,EACxD;AAID,QAAM,iBAAiB,WAAW,UAAU,aAAa,aAAa,GAAG,IAAI;AAC7E,MAAI,iBAAiB,YAAY,aAAa,GAAG;AAC/C,UAAM,IAAI,MAAM,2BAA2B,2CAA2C,YAAY,aAAa,GAAG;AAAA,EACnH;AAGD,MAAI,iBAAiB,IAAI,kBAAkB;AAEzC,UAAM,iBAAiB,YAAY,aAAa,iBAAiB;AACjE,UAAM,iBAAiB,MAAM,YAAY,MAAM,gBAAgB,YAAY;AAE3E,UAAM,iBAAiB,IAAI,YAAY,iBAAiB,CAAC;AACzD,UAAM,eAAe,IAAI,WAAW,cAAc;AAClD,iBAAa,IAAI,IAAI,WAAW,cAAc,CAAC;AAC/C,iBAAa,IAAI,IAAI,WAAW,YAAY,GAAG,eAAe,cAAc;AAC5E,WAAO,gBAAgB,gBAAgB,EAAE,SAAS,WAAU,CAAE;AAAA,EAClE,OAAS;AAEL,WAAO,gBAAgB,cAAc,EAAE,SAAS,WAAU,CAAE;AAAA,EAC7D;AACH;AAUO,SAAS,gBAAgB,aAAa,EAAE,SAAS,aAAa,KAAM,IAAG,IAAI;AAChF,MAAI,EAAE,uBAAuB;AAAc,UAAM,IAAI,MAAM,8BAA8B;AACzF,QAAM,OAAO,IAAI,SAAS,WAAW;AAGrC,YAAU,EAAE,GAAG,iBAAiB,GAAG,QAAS;AAG5C,MAAI,KAAK,aAAa,GAAG;AACvB,UAAM,IAAI,MAAM,2BAA2B;AAAA,EAC5C;AACD,MAAI,KAAK,UAAU,KAAK,aAAa,GAAG,IAAI,MAAM,WAAY;AAC5D,UAAM,IAAI,MAAM,uCAAuC;AAAA,EACxD;AAID,QAAM,uBAAuB,KAAK,aAAa;AAC/C,QAAM,iBAAiB,KAAK,UAAU,sBAAsB,IAAI;AAChE,MAAI,iBAAiB,KAAK,aAAa,GAAG;AAExC,UAAM,IAAI,MAAM,2BAA2B,2CAA2C,KAAK,aAAa,GAAG;AAAA,EAC5G;AAED,QAAM,iBAAiB,uBAAuB;AAC9C,QAAM,SAAS,EAAE,MAAM,QAAQ,eAAgB;AAC/C,QAAM,WAAW,4BAA4B,MAAM;AAGnD,QAAM,UAAU,SAAS;AAEzB,QAAM,SAAS,SAAS,QAAQ,IAAI,CAAoB,WAAW;AAAA,IACjE,MAAM,aAAa,MAAM;AAAA,IACzB,aAAa,MAAM;AAAA,IACnB,iBAAiB,qBAAqB,MAAM;AAAA,IAC5C,MAAM,OAAO,MAAM,OAAO;AAAA,IAC1B,cAAc,MAAM;AAAA,IACpB,gBAAgB,eAAe,MAAM;AAAA,IACrC,OAAO,MAAM;AAAA,IACb,WAAW,MAAM;AAAA,IACjB,UAAU,MAAM;AAAA,IAChB,cAAc,YAAY,MAAM,QAAQ;AAAA,EAC5C,EAAI;AAEF,QAAM,eAAe,OAAO,OAAO,OAAK,EAAE,IAAI;AAC9C,QAAM,WAAW,SAAS;AAC1B,QAAM,aAAa,SAAS,QAAQ,IAAI,CAAoB,cAAc;AAAA,IACxE,SAAS,SAAS,QAAQ,IAAI,CAAoB,QAA8B,iBAAiB;AAAA,MAC/F,WAAW,OAAO,OAAO,OAAO;AAAA,MAChC,aAAa,OAAO;AAAA,MACpB,WAAW,OAAO,WAAW;AAAA,QAC3B,MAAM,aAAa,OAAO,QAAQ;AAAA,QAClC,WAAW,OAAO,QAAQ,SAAS,IAAI,CAAuB,MAAM,UAAU,EAAE;AAAA,QAChF,gBAAgB,OAAO,QAAQ,QAAQ,IAAI,MAAM;AAAA,QACjD,OAAO,kBAAkB,OAAO,QAAQ;AAAA,QACxC,YAAY,OAAO,QAAQ;AAAA,QAC3B,yBAAyB,OAAO,QAAQ;AAAA,QACxC,uBAAuB,OAAO,QAAQ;AAAA,QACtC,oBAAoB,OAAO,QAAQ,SAAS,IAAI,CAAoB,QAAQ;AAAA,UAC1E,KAAK,OAAO,GAAG,OAAO;AAAA,UACtB,OAAO,OAAO,GAAG,OAAO;AAAA,QAClC,EAAU;AAAA,QACF,kBAAkB,OAAO,QAAQ;AAAA,QACjC,mBAAmB,OAAO,QAAQ;AAAA,QAClC,wBAAwB,OAAO,QAAQ;AAAA,QACvC,YAAY,aAAa,OAAO,QAAQ,UAAU,aAAa,cAAc,OAAO;AAAA,QACpF,gBAAgB,OAAO,QAAQ,UAAU,IAAI,CAAoB,kBAAkB;AAAA,UACjF,WAAW,UAAU,aAAa;AAAA,UAClC,UAAU,UAAU,aAAa;AAAA,UACjC,OAAO,aAAa;AAAA,QAC9B,EAAU;AAAA,QACF,qBAAqB,OAAO,QAAQ;AAAA,QACpC,qBAAqB,OAAO,QAAQ;AAAA,QACpC,iBAAiB,OAAO,QAAQ,YAAY;AAAA,UAC1C,iCAAiC,OAAO,QAAQ,SAAS;AAAA,UACzD,4BAA4B,OAAO,QAAQ,SAAS;AAAA,UACpD,4BAA4B,OAAO,QAAQ,SAAS;AAAA,QACrD;AAAA,QACD,uBAAuB,OAAO,QAAQ,YAAY;AAAA,UAChD,MAAM,OAAO,QAAQ,SAAS,WAAW;AAAA,YACvC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,UACvC;AAAA,UACD,kBAAkB,OAAO,QAAQ,SAAS;AAAA,QAC3C;AAAA,MACF;AAAA,MACD,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,iBAAiB,OAAO;AAAA,MACxB,2BAA2B,OAAO;AAAA,IACxC,EAAM;AAAA,IACF,iBAAiB,SAAS;AAAA,IAC1B,UAAU,SAAS;AAAA,IACnB,iBAAiB,SAAS,SAAS,IAAI,CAAoB,mBAAmB;AAAA,MAC5E,YAAY,cAAc;AAAA,MAC1B,YAAY,cAAc;AAAA,MAC1B,aAAa,cAAc;AAAA,IACjC,EAAM;AAAA,IACF,aAAa,SAAS;AAAA,IACtB,uBAAuB,SAAS;AAAA,IAChC,SAAS,SAAS;AAAA,EACtB,EAAI;AAEF,QAAM,qBAAqB,SAAS,SAAS,IAAI,CAAoB,QAAQ;AAAA,IAC3E,KAAK,OAAO,GAAG,OAAO;AAAA,IACtB,OAAO,OAAO,GAAG,OAAO;AAAA,EAC5B,EAAI;AACF,QAAM,aAAa,OAAO,SAAS,OAAO;AAE1C,MAAI,YAAY;AACd,mBAAe,QAAQ,kBAAkB;AAAA,EAC1C;AAED,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,iBAAiB;AAAA,EAClB;AACH;AAQO,SAAS,cAAc,EAAE,UAAU;AACxC,SAAO,cAAc,QAAQ,CAAE,CAAA,EAAE;AACnC;AAMA,SAAS,YAAYC,cAAa;AAChC,MAAIA,cAAa;AAAS,WAAO,EAAE,MAAM,SAAU;AACnD,MAAIA,cAAa;AAAS,WAAO,EAAE,MAAM,MAAO;AAChD,MAAIA,cAAa;AAAS,WAAO,EAAE,MAAM,OAAQ;AACjD,MAAIA,cAAa;AAAS,WAAO,EAAE,MAAM,OAAQ;AACjD,MAAIA,cAAa;AAAS,WAAO;AAAA,MAC/B,MAAM;AAAA,MACN,OAAOA,aAAY,QAAQ;AAAA,MAC3B,WAAWA,aAAY,QAAQ;AAAA,IAChC;AACD,MAAIA,cAAa;AAAS,WAAO,EAAE,MAAM,OAAQ;AACjD,MAAIA,cAAa;AAAS,WAAO;AAAA,MAC/B,MAAM;AAAA,MACN,iBAAiBA,aAAY,QAAQ;AAAA,MACrC,MAAM,SAASA,aAAY,QAAQ,OAAO;AAAA,IAC3C;AACD,MAAIA,cAAa;AAAS,WAAO;AAAA,MAC/B,MAAM;AAAA,MACN,iBAAiBA,aAAY,QAAQ;AAAA,MACrC,MAAM,SAASA,aAAY,QAAQ,OAAO;AAAA,IAC3C;AACD,MAAIA,cAAa;AAAU,WAAO;AAAA,MAChC,MAAM;AAAA,MACN,UAAUA,aAAY,SAAS;AAAA,MAC/B,UAAUA,aAAY,SAAS;AAAA,IAChC;AACD,MAAIA,cAAa;AAAU,WAAO,EAAE,MAAM,OAAQ;AAClD,MAAIA,cAAa;AAAU,WAAO,EAAE,MAAM,OAAQ;AAClD,MAAIA,cAAa;AAAU,WAAO,EAAE,MAAM,OAAQ;AAClD,MAAIA,cAAa;AAAU,WAAO,EAAE,MAAM,OAAQ;AAClD,MAAIA,cAAa;AAAU,WAAO,EAAE,MAAM,UAAW;AACrD,MAAIA,cAAa;AAAU,WAAO;AAAA,MAChC,MAAM;AAAA,MACN,uBAAuBA,aAAY,SAAS;AAAA,IAC7C;AACD,MAAIA,cAAa;AAAU,WAAO;AAAA,MAChC,MAAM;AAAA,MACN,KAAK,OAAOA,aAAY,SAAS,OAAO;AAAA,IACzC;AACD,MAAIA,cAAa;AAAU,WAAO;AAAA,MAChC,MAAM;AAAA,MACN,KAAK,OAAOA,aAAY,SAAS,OAAO;AAAA,MACxC,WAAW,4BAA4BA,aAAY,SAAS;AAAA,IAC7D;AACD,SAAOA;AACT;AAMA,SAAS,SAAS,MAAM;AACtB,MAAI,KAAK;AAAS,WAAO;AACzB,MAAI,KAAK;AAAS,WAAO;AACzB,MAAI,KAAK;AAAS,WAAO;AACzB,QAAM,IAAI,MAAM,4BAA4B;AAC9C;AAWA,SAAS,aAAa,OAAO,QAAQ,SAAS;AAC5C,SAAO,SAAS;AAAA,IACd,KAAK,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACnD,KAAK,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACnD,YAAY,MAAM;AAAA,IAClB,gBAAgB,MAAM;AAAA,IACtB,WAAW,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACzD,WAAW,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACzD,oBAAoB,MAAM;AAAA,IAC1B,oBAAoB,MAAM;AAAA,EAC3B;AACH;AAQO,SAAS,gBAAgB,OAAO,QAAQ,SAAS;AACtD,QAAM,EAAE,MAAM,gBAAgB,aAAc,IAAG;AAC/C,MAAI,UAAU;AAAW,WAAO;AAChC,MAAI,SAAS;AAAW,WAAO,MAAM,OAAO;AAC5C,MAAI,SAAS;AAAc,WAAO,QAAQ,gBAAgB,KAAK;AAC/D,QAAM,OAAO,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU;AAC1E,MAAI,SAAS,WAAW,KAAK,eAAe;AAAG,WAAO,KAAK,WAAW,GAAG,IAAI;AAC7E,MAAI,SAAS,YAAY,KAAK,eAAe;AAAG,WAAO,KAAK,WAAW,GAAG,IAAI;AAC9E,MAAI,SAAS,WAAW,mBAAmB;AAAQ,WAAO,QAAQ,aAAa,KAAK,SAAS,GAAG,IAAI,CAAC;AACrG,MAAI,SAAS,WAAW,mBAAmB;AAAoB,WAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjI,MAAI,SAAS,WAAW,mBAAmB;AAAoB,WAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjI,MAAI,SAAS,WAAW,cAAc,SAAS,eAAe,cAAc,SAAS;AAAS,WAAO,QAAQ,yBAAyB,KAAK,YAAY,GAAG,IAAI,CAAC;AAC/J,MAAI,SAAS,WAAW,cAAc,SAAS,eAAe,cAAc,SAAS;AAAU,WAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjK,MAAI,SAAS,WAAW,cAAc,SAAS;AAAa,WAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AAC9H,MAAI,SAAS,WAAW,KAAK,eAAe;AAAG,WAAO,KAAK,SAAS,GAAG,IAAI;AAC3E,MAAI,SAAS,WAAW,KAAK,eAAe;AAAG,WAAO,KAAK,YAAY,GAAG,IAAI;AAC9E,MAAI,mBAAmB;AAAW,WAAO,aAAa,KAAK,IAAI,MAAM,EAAE,OAAO,SAAS;AACvF,MAAI,cAAc,SAAS;AAAW,WAAO,aAAa,KAAK;AAC/D,MAAI,SAAS;AAAwB,WAAO;AAE5C,SAAO;AACT;AC/SO,SAAS,OAAO,KAAK,KAAK;AAC/B,QAAM,QAAQ;AACd,WAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK,OAAO;AAC1C,QAAI,KAAK,GAAG,IAAI,MAAM,GAAG,IAAI,KAAK,CAAC;AAAA,EACpC;AACH;AASO,SAAS,OAAO,GAAG,GAAG;AAC3B,MAAI,MAAM;AAAG,WAAO;AACpB,MAAI,aAAa,cAAc,aAAa;AAAY,WAAO,OAAO,MAAM,KAAK,CAAC,GAAG,MAAM,KAAK,CAAC,CAAC;AAClG,MAAI,CAAC,KAAK,CAAC,KAAK,OAAO,MAAM,OAAO;AAAG,WAAO;AAC9C,SAAO,MAAM,QAAQ,CAAC,KAAK,MAAM,QAAQ,CAAC,IACtC,EAAE,WAAW,EAAE,UAAU,EAAE,MAAM,CAAC,GAAG,MAAM,OAAO,GAAG,EAAE,EAAE,CAAC,IAC1D,OAAO,MAAM,YAAY,OAAO,KAAK,CAAC,EAAE,WAAW,OAAO,KAAK,CAAC,EAAE,UAAU,OAAO,KAAK,CAAC,EAAE,MAAM,OAAK,OAAO,EAAE,IAAI,EAAE,EAAE,CAAC;AAC9H;AA2MO,SAAS,QAAQ,QAAQ;AAC9B,MAAI,CAAC;AAAQ,WAAO,CAAE;AACtB,MAAI,OAAO,WAAW;AAAG,WAAO,OAAO;AAEvC,QAAM,SAAS,CAAE;AACjB,aAAW,SAAS,QAAQ;AAC1B,WAAO,QAAQ,KAAK;AAAA,EACrB;AACD,SAAO;AACT;ACpNO,SAAS,gBAAgB,QAAQ,OAAO,iBAAiB;AAC9D,MAAI,CAAC;AAAQ,WAAO;AAGpB,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAElD,WAAO,OAAO,KAAK,KAAK,eAAa,gBAAgB,WAAW,OAAO,eAAe,CAAC;AAAA,EACxF;AACD,MAAI,SAAS,UAAU,MAAM,QAAQ,OAAO,GAAG,GAAG;AAEhD,WAAO,OAAO,IAAI,MAAM,eAAa,gBAAgB,WAAW,OAAO,eAAe,CAAC;AAAA,EACxF;AACD,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAGlD,WAAO;AAAA,EACR;AAGD,aAAW,CAAC,OAAO,SAAS,KAAK,OAAO,QAAQ,MAAM,GAAG;AAEvD,UAAM,cAAc,gBAAgB,QAAQ,KAAK;AACjD,QAAI,gBAAgB;AAAI;AAExB,UAAM,cAAc,MAAM,QAAQ;AAClC,UAAM,QAAQ,YAAY,WAAW;AACrC,QAAI,CAAC;AAAO;AAEZ,UAAM,EAAE,KAAK,KAAK,WAAW,UAAW,IAAG;AAC3C,UAAM,SAAS,cAAc,SAAY,YAAY;AACrD,UAAM,SAAS,cAAc,SAAY,YAAY;AAErD,QAAI,WAAW,UAAa,WAAW;AAAW;AAGlD,eAAW,CAAC,UAAU,MAAM,KAAK,OAAO,QAAQ,aAAa,CAAA,CAAE,GAAG;AAChE,UAAI,aAAa,SAAS,UAAU;AAAQ,eAAO;AACnD,UAAI,aAAa,UAAU,SAAS;AAAQ,eAAO;AACnD,UAAI,aAAa,SAAS,UAAU;AAAQ,eAAO;AACnD,UAAI,aAAa,UAAU,SAAS;AAAQ,eAAO;AACnD,UAAI,aAAa,UAAU,SAAS,UAAU,SAAS;AAAS,eAAO;AACvE,UAAI,aAAa,SAAS,OAAO,QAAQ,MAAM,KAAK,OAAO,QAAQ,MAAM;AAAG,eAAO;AACnF,UAAI,aAAa,SAAS,MAAM,QAAQ,MAAM,KAAK,OAAO,MAAM,OAAK,IAAI,UAAU,IAAI,MAAM;AAAG,eAAO;AACvG,UAAI,aAAa,UAAU,MAAM,QAAQ,MAAM,KAAK,OAAO,QAAQ,MAAM,KAAK,OAAO,SAAS,MAAM;AAAG,eAAO;AAAA,IAC/G;AAAA,EACF;AAED,SAAO;AACT;AC/FA,MAAM,yBAAyB,KAAK;AAY7B,SAAS,YAAY,EAAE,UAAU,WAAW,GAAG,SAAS,UAAU,SAAS,UAAU;AAC1F,MAAI,CAAC;AAAU,UAAM,IAAI,MAAM,+BAA+B;AAE9D,QAAM,SAAS,CAAE;AAEjB,QAAM,UAAU,CAAE;AAClB,QAAM,kBAAkB,mBAAmB,cAAc,QAAQ,CAAC;AAGlE,MAAI,aAAa;AACjB,aAAW,YAAY,SAAS,YAAY;AAC1C,UAAM,YAAY,OAAO,SAAS,QAAQ;AAC1C,UAAM,WAAW,aAAa;AAE9B,QAAI,YAAY,KAAK,WAAW,YAAY,aAAa,UAAU,CAAC,gBAAgB,QAAQ,UAAU,eAAe,GAAG;AAEtH,YAAM,SAAS,CAAE;AAEjB,iBAAW,EAAE,WAAW,UAAS,KAAM,SAAS,SAAS;AACvD,YAAI;AAAW,gBAAM,IAAI,MAAM,iCAAiC;AAChE,YAAI,CAAC;AAAW,gBAAM,IAAI,MAAM,sCAAsC;AAEtE,YAAI,CAAC,WAAW,QAAQ,SAAS,UAAU,eAAe,EAAE,GAAG;AAC7D,iBAAO,KAAK,eAAe,SAAS,CAAC;AAAA,QACtC;AAAA,MACF;AACD,YAAM,cAAc,KAAK,IAAI,WAAW,YAAY,CAAC;AACrD,YAAM,YAAY,KAAK,IAAI,SAAS,YAAY,SAAS;AACzD,aAAO,KAAK,EAAE,QAAQ,UAAU,YAAY,WAAW,aAAa,WAAW;AAG/E,YAAM,YAAY,OAAO,OAAO,SAAS,IAAI,UAAU,OAAO,IAAI;AAClE,UAAI,CAAC,WAAW,YAAY,wBAAwB;AAElD,gBAAQ,KAAK;AAAA,UACX,WAAW,OAAO,GAAG;AAAA,UACrB,SAAS,OAAO,OAAO,SAAS,GAAG;AAAA,QAC7C,CAAS;AAAA,MACT,WAAiB,OAAO,QAAQ;AACxB,eAAO,SAAS,MAAM;AAAA,MAC9B,WAAiB,SAAS,QAAQ;AAC1B,cAAM,IAAI,MAAM,8BAA8B,QAAQ,KAAK,IAAI,GAAG;AAAA,MACnE;AAAA,IACF;AAED,iBAAa;AAAA,EACd;AACD,MAAI,CAAC,SAAS,MAAM;AAAG,aAAS;AAEhC,SAAO,EAAE,UAAU,UAAU,QAAQ,SAAS,SAAS,OAAQ;AACjE;AAMO,SAAS,eAAe,EAAE,wBAAwB,kBAAkB,sBAAqB,GAAI;AAClG,QAAM,eAAe,0BAA0B;AAC/C,SAAO;AAAA,IACL,WAAW,OAAO,YAAY;AAAA,IAC9B,SAAS,OAAO,eAAe,qBAAqB;AAAA,EACrD;AACH;AASO,SAAS,oBAAoB,MAAM,EAAE,WAAW;AAErD,QAAM,WAAW,QAAQ,IAAI,CAAC,EAAE,WAAW,QAAS,MAAK,KAAK,MAAM,WAAW,OAAO,CAAC;AACvF,SAAO;AAAA,IACL,YAAY,KAAK;AAAA,IACjB,MAAM,OAAO,MAAM,KAAK,YAAY;AAElC,YAAM,QAAQ,QAAQ,UAAU,CAAC,EAAE,WAAW,QAAO,MAAO,aAAa,SAAS,OAAO,OAAO;AAChG,UAAI,QAAQ;AAAG,cAAM,IAAI,MAAM,0BAA0B,UAAU,MAAM;AACzE,UAAI,QAAQ,OAAO,cAAc,SAAS,QAAQ,OAAO,YAAY,KAAK;AAExE,cAAM,cAAc,QAAQ,QAAQ,OAAO;AAC3C,cAAM,YAAY,MAAM,QAAQ,OAAO;AACvC,YAAI,SAAS,kBAAkB,SAAS;AACtC,iBAAO,SAAS,OAAO,KAAK,YAAU,OAAO,MAAM,aAAa,SAAS,CAAC;AAAA,QACpF,OAAe;AACL,iBAAO,SAAS,OAAO,MAAM,aAAa,SAAS;AAAA,QACpD;AAAA,MACT,OAAa;AACL,eAAO,SAAS;AAAA,MACjB;AAAA,IACF;AAAA,EACF;AACH;ACnGO,SAAS,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,YAAY;AAC5F,QAAM,IAAI,kBAAkB,UAAU,iBAAiB;AACvD,MAAI,CAAC;AAAG,WAAO;AACf,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,QAAM,iBAAiB,WAAW,IAAI,CAAC,EAAE,QAAS,MAAK,QAAQ,eAAe;AAC9E,MAAI,aAAa;AAGjB,QAAM,iBAAiB,CAAC,MAAM;AAC9B,MAAI,mBAAmB;AACvB,MAAI,eAAe;AACnB,MAAI,kBAAkB;AACtB,MAAI,kBAAkB;AAEtB,MAAI,iBAAiB,IAAI;AAEvB,WAAO,eAAe,eAAe,SAAS,KAAK,kBAAkB,iBAAiB,IAAI;AACxF;AACA,UAAI,eAAe,kBAAkB,YAAY;AAE/C,2BAAmB,iBAAiB,GAAG,EAAE;AACzC,uBAAe,KAAK,gBAAgB;AACpC;AAAA,MACD;AACD,UAAI,eAAe,kBAAkB;AAAY;AAAA,IAClD;AAAA,EACF;AAED,WAAS,IAAI,GAAG,IAAI,GAAG,KAAK;AAE1B,UAAM,MAAM,kBAAkB,SAAS,iBAAiB,KAAK;AAC7D,UAAM,MAAM,iBAAiB;AAG7B,WAAO,iBAAiB,MAAM,mBAAmB,eAAe,kBAAkB,aAAa;AAC7F,UAAI,eAAe,kBAAkB,YAAY;AAC/C,uBAAe,IAAK;AACpB;AAAA,MACD;AACD,UAAI,eAAe,kBAAkB;AAAY;AACjD;AAAA,IACD;AAED,uBAAmB,eAAe,GAAG,EAAE;AAGvC,YACG,eAAe,eAAe,SAAS,KAAK,eAAe,eAAe,OAAO,gBACjF,kBAAkB,OAAO,eAAe,eAAe,OAAO,aAC/D;AACA;AACA,UAAI,eAAe,kBAAkB,YAAY;AAE/C,cAAM,UAAU,CAAE;AAClB,yBAAiB,KAAK,OAAO;AAC7B,2BAAmB;AACnB,uBAAe,KAAK,OAAO;AAC3B;AAAA,MACD;AACD,UAAI,eAAe,kBAAkB;AAAY;AAAA,IAClD;AAGD,QAAI,QAAQ,oBAAoB;AAE9B,uBAAiB,KAAK,OAAO,aAAa;AAAA,IAC3C,WAAU,iBAAiB,eAAe,SAAS,GAAG;AACrD,uBAAiB,KAAK,IAAI;AAAA,IAChC,OAAW;AACL,uBAAiB,KAAK,EAAE;AAAA,IACzB;AAAA,EACF;AAGD,MAAI,CAAC,OAAO,QAAQ;AAElB,aAAS,IAAI,GAAG,IAAI,oBAAoB,KAAK;AAE3C,YAAM,UAAU,CAAE;AAClB,uBAAiB,KAAK,OAAO;AAC7B,yBAAmB;AAAA,IACpB;AAAA,EACF;AAED,SAAO;AACT;AAUO,SAAS,eAAe,eAAe,QAAQ,QAAQ,GAAG;AAC/D,QAAM,OAAO,OAAO,KAAK,KAAK,GAAG;AACjC,QAAM,WAAW,OAAO,QAAQ,oBAAoB;AACpD,QAAM,YAAY,WAAW,QAAQ,IAAI;AAEzC,MAAI,WAAW,MAAM,GAAG;AACtB,QAAI,UAAU,OAAO,SAAS;AAC9B,QAAI,WAAW;AACf,QAAI,QAAQ,SAAS,WAAW,GAAG;AACjC,gBAAU,QAAQ,SAAS;AAC3B;AAAA,IACD;AACD,mBAAe,eAAe,SAAS,QAAQ;AAE/C,UAAM,YAAY,QAAQ,KAAK,KAAK,GAAG;AACvC,UAAM,SAAS,cAAc,IAAI,SAAS;AAC1C,QAAI,CAAC;AAAQ,YAAM,IAAI,MAAM,oCAAoC;AACjE,QAAI;AAAU,qBAAe,QAAQ,KAAK;AAC1C,kBAAc,IAAI,MAAM,MAAM;AAC9B,kBAAc,OAAO,SAAS;AAC9B;AAAA,EACD;AAED,MAAI,UAAU,MAAM,GAAG;AACrB,UAAM,UAAU,OAAO,SAAS,GAAG,QAAQ;AAG3C,mBAAe,eAAe,OAAO,SAAS,GAAG,SAAS,IAAI,YAAY,CAAC;AAC3E,mBAAe,eAAe,OAAO,SAAS,GAAG,SAAS,IAAI,YAAY,CAAC;AAE3E,UAAM,OAAO,cAAc,IAAI,GAAG,QAAQ,aAAa;AACvD,UAAM,SAAS,cAAc,IAAI,GAAG,QAAQ,eAAe;AAE3D,QAAI,CAAC;AAAM,YAAM,IAAI,MAAM,iCAAiC;AAC5D,QAAI,CAAC;AAAQ,YAAM,IAAI,MAAM,mCAAmC;AAChE,QAAI,KAAK,WAAW,OAAO,QAAQ;AACjC,YAAM,IAAI,MAAM,8CAA8C;AAAA,IAC/D;AAED,UAAM,MAAM,aAAa,MAAM,QAAQ,SAAS;AAChD,QAAI;AAAU,qBAAe,KAAK,KAAK;AAEvC,kBAAc,OAAO,GAAG,QAAQ,aAAa;AAC7C,kBAAc,OAAO,GAAG,QAAQ,eAAe;AAC/C,kBAAc,IAAI,MAAM,GAAG;AAC3B;AAAA,EACD;AAGD,MAAI,OAAO,SAAS,QAAQ;AAE1B,UAAM,cAAc,OAAO,QAAQ,oBAAoB,aAAa,QAAQ,QAAQ;AAEpF,UAAM,SAAS,CAAE;AACjB,eAAW,SAAS,OAAO,UAAU;AACnC,qBAAe,eAAe,OAAO,WAAW;AAChD,YAAM,YAAY,cAAc,IAAI,MAAM,KAAK,KAAK,GAAG,CAAC;AACxD,UAAI,CAAC;AAAW,cAAM,IAAI,MAAM,mCAAmC;AACnE,aAAO,MAAM,QAAQ,QAAQ;AAAA,IAC9B;AAED,eAAW,SAAS,OAAO,UAAU;AACnC,oBAAc,OAAO,MAAM,KAAK,KAAK,GAAG,CAAC;AAAA,IAC1C;AAED,UAAM,WAAW,aAAa,QAAQ,WAAW;AACjD,QAAI;AAAU,qBAAe,UAAU,KAAK;AAC5C,kBAAc,IAAI,MAAM,QAAQ;AAAA,EACjC;AACH;AAOA,SAAS,eAAe,KAAK,OAAO;AAClC,WAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,QAAI,OAAO;AACT,qBAAe,IAAI,IAAI,QAAQ,CAAC;AAAA,IACtC,OAAW;AACL,UAAI,KAAK,IAAI,GAAG;AAAA,IACjB;AAAA,EACF;AACH;AAQA,SAAS,aAAa,MAAM,QAAQ,OAAO;AACzC,QAAM,MAAM,CAAE;AACd,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,KAAK;AACpC,QAAI,OAAO;AACT,UAAI,KAAK,aAAa,KAAK,IAAI,OAAO,IAAI,QAAQ,CAAC,CAAC;AAAA,IAC1D,OAAW;AACL,UAAI,KAAK,IAAI;AAEX,cAAM,MAAM,CAAE;AACd,iBAAS,IAAI,GAAG,IAAI,KAAK,GAAG,QAAQ,KAAK;AACvC,gBAAM,QAAQ,OAAO,GAAG;AACxB,cAAI,KAAK,GAAG,MAAM,UAAU,SAAY,OAAO;AAAA,QAChD;AACD,YAAI,KAAK,GAAG;AAAA,MACpB,OAAa;AACL,YAAI,KAAK,MAAS;AAAA,MACnB;AAAA,IACF;AAAA,EACF;AACD,SAAO;AACT;AASA,SAAS,aAAa,QAAQ,OAAO;AACnC,QAAM,OAAO,OAAO,KAAK,MAAM;AAC/B,QAAM,SAAS,OAAO,KAAK,KAAK;AAChC,QAAM,MAAM,CAAE;AACd,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAE/B,UAAM,MAAM,CAAE;AACd,eAAW,OAAO,MAAM;AACtB,UAAI,OAAO,KAAK,WAAW;AAAQ,cAAM,IAAI,MAAM,8BAA8B;AACjF,UAAI,OAAO,OAAO,KAAK;AAAA,IACxB;AACD,QAAI,OAAO;AACT,UAAI,KAAK,aAAa,KAAK,QAAQ,CAAC,CAAC;AAAA,IAC3C,OAAW;AACL,UAAI,KAAK,GAAG;AAAA,IACb;AAAA,EACF;AACD,SAAO;AACT;AC/OO,SAAS,kBAAkB,QAAQ,OAAO,QAAQ;AACvD,QAAM,QAAQ,kBAAkB;AAChC,QAAM,YAAY,WAAW,MAAM;AACnC,QAAM,oBAAoB,WAAW,MAAM;AAC3C,aAAW,MAAM;AACjB,MAAI,QAAQ,iBAAiB,MAAM;AACnC,MAAI,cAAc;AAClB,SAAO,iBAAiB,QAAQ,OAAO,KAAK,IAAI;AAEhD,QAAM,qBAAqB,YAAY;AAEvC,SAAO,cAAc,OAAO;AAE1B,UAAM,WAAW,iBAAiB,MAAM;AACxC,UAAM,YAAY,IAAI,WAAW,iBAAiB;AAClD,aAAS,IAAI,GAAG,IAAI,mBAAmB,KAAK;AAC1C,gBAAU,KAAK,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,IACpD;AAED,aAAS,IAAI,GAAG,IAAI,qBAAqB,cAAc,OAAO,KAAK;AAEjE,YAAMC,YAAW,OAAO,UAAU,EAAE;AACpC,UAAIA,WAAU;AACZ,YAAI,aAAa;AACjB,YAAI,iBAAiB;AACrB,cAAM,QAAQ,MAAMA,aAAY;AAChC,eAAO,kBAAkB,cAAc,OAAO;AAC5C,cAAI,OAAO,OAAO,OAAO,KAAK,SAAS,OAAO,MAAM,CAAC,KAAK,aAAa;AACvE,wBAAcA;AACd,iBAAO,cAAc,GAAG;AACtB,0BAAc;AACd,mBAAO;AACP,gBAAI,YAAY;AACd,sBAAQ,OAAO,OAAO,KAAK,SAAS,OAAO,MAAM,CAAC,KAAKA,YAAW,aAAa;AAAA,YAChF;AAAA,UACF;AACD,gBAAM,QAAQ,WAAW;AACzB,mBAAS;AACT,iBAAO,iBAAiB,QAAQ,OAAO,KAAK,IAAI;AAChD;AAAA,QACD;AACD,YAAI,gBAAgB;AAElB,iBAAO,UAAU,KAAK,MAAM,iBAAiB,OAAOA,SAAQ,IAAI,OAAO,UAAU,KAAK,CAAC;AAAA,QACxF;AAAA,MACT,OAAa;AACL,iBAAS,IAAI,GAAG,IAAI,sBAAsB,cAAc,OAAO,KAAK;AAClE,mBAAS;AACT,iBAAO,iBAAiB,QAAQ,OAAO,KAAK,IAAI;AAAA,QACjD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACH;AAOO,SAAS,qBAAqB,QAAQ,OAAO,QAAQ;AAC1D,QAAM,UAAU,IAAI,WAAW,KAAK;AACpC,oBAAkB,QAAQ,OAAO,OAAO;AACxC,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,KAAK,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,EAAE;AACjG,WAAO,UAAU,QAAQ;AAAA,EAC1B;AACH;AAOO,SAAS,eAAe,QAAQ,OAAO,QAAQ;AACpD,QAAM,aAAa,IAAI,WAAW,KAAK;AACvC,oBAAkB,QAAQ,OAAO,UAAU;AAC3C,QAAM,aAAa,IAAI,WAAW,KAAK;AACvC,oBAAkB,QAAQ,OAAO,UAAU;AAE3C,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,SAAS,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,WAAW,EAAE;AACvG,QAAI,WAAW,IAAI;AAEjB,aAAO,KAAK,IAAI,WAAW,WAAW,KAAK,WAAW,EAAE;AACxD,aAAO,GAAG,IAAI,OAAO,IAAI,GAAG,SAAS,GAAG,WAAW,EAAE,CAAC;AACtD,aAAO,GAAG,IAAI,QAAQ,WAAW,EAAE;AAAA,IACzC,OAAW;AACL,aAAO,KAAK;AAAA,IACb;AACD,WAAO,UAAU,WAAW;AAAA,EAC7B;AACH;AC5FO,SAAS,SAAS,OAAO;AAC9B,SAAO,KAAK,KAAK,MAAM,KAAK;AAC9B;AAYO,SAAS,uBAAuB,QAAQ,OAAO,QAAQ,QAAQ;AACpE,MAAI,WAAW,QAAW;AACxB,aAAS,OAAO,KAAK,UAAU,OAAO,QAAQ,IAAI;AAClD,WAAO,UAAU;AAAA,EAClB;AACD,QAAM,cAAc,OAAO;AAC3B,MAAI,OAAO;AACX,SAAO,OAAO,OAAO,QAAQ;AAC3B,UAAM,SAAS,WAAW,MAAM;AAChC,QAAI,SAAS,GAAG;AAEd,aAAO,cAAc,QAAQ,QAAQ,OAAO,QAAQ,IAAI;AAAA,IAC9D,OAAW;AAEL,YAAM,QAAQ,WAAW;AACzB,cAAQ,QAAQ,OAAO,OAAO,QAAQ,IAAI;AAC1C,cAAQ;AAAA,IACT;AAAA,EACF;AACD,SAAO,SAAS,cAAc;AAChC;AAWA,SAAS,QAAQ,QAAQ,OAAOA,WAAU,QAAQ,MAAM;AACtD,QAAM,QAAQA,YAAW,KAAK;AAC9B,MAAI,QAAQ;AACZ,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,aAAS,OAAO,KAAK,SAAS,OAAO,QAAQ,MAAM,KAAK;AAAA,EACzD;AAID,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,OAAO,KAAK;AAAA,EACpB;AACH;AAaA,SAAS,cAAc,QAAQ,QAAQA,WAAU,QAAQ,MAAM;AAC7D,MAAI,QAAQ,UAAU,KAAK;AAC3B,QAAM,QAAQ,KAAKA,aAAY;AAE/B,MAAI,OAAO;AACX,MAAI,OAAO,SAAS,OAAO,KAAK,YAAY;AAC1C,WAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,EAC5C,WAAU,MAAM;AAEf,UAAM,IAAI,MAAM,0BAA0B,OAAO,qBAAqB;AAAA,EACvE;AACD,MAAI,OAAO;AACX,MAAI,QAAQ;AAGZ,SAAO,OAAO;AAEZ,QAAI,QAAQ,GAAG;AACb,eAAS;AACT,cAAQ;AACR,gBAAU;AAAA,IAChB,WAAe,OAAO,QAAQA,WAAU;AAElC,cAAQ,OAAO,KAAK,SAAS,OAAO,MAAM,KAAK;AAC/C,aAAO;AACP,cAAQ;AAAA,IACd,OAAW;AACL,UAAI,OAAO,OAAO,QAAQ;AAExB,eAAO,UAAU,QAAQ,QAAQ;AAAA,MAClC;AACD;AACA,eAASA;AAAA,IACV;AAAA,EACF;AAED,SAAO;AACT;AASO,SAAS,gBAAgB,QAAQ,OAAO,MAAM,YAAY;AAC/D,QAAM,QAAQ,UAAU,MAAM,UAAU;AACxC,QAAM,QAAQ,IAAI,WAAW,QAAQ,KAAK;AAC1C,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,aAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,YAAM,IAAI,QAAQ,KAAK,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,IAC5D;AAAA,EACF;AAED,MAAI,SAAS;AAAS,WAAO,IAAI,aAAa,MAAM,MAAM;AAAA,WACjD,SAAS;AAAU,WAAO,IAAI,aAAa,MAAM,MAAM;AAAA,WACvD,SAAS;AAAS,WAAO,IAAI,WAAW,MAAM,MAAM;AAAA,WACpD,SAAS;AAAS,WAAO,IAAI,cAAc,MAAM,MAAM;AAAA,WACvD,SAAS,wBAAwB;AAExC,UAAM,QAAQ,IAAI,MAAM,KAAK;AAC7B,aAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,YAAM,KAAK,MAAM,SAAS,IAAI,QAAQ,IAAI,KAAK,KAAK;AAAA,IACrD;AACD,WAAO;AAAA,EACR;AACD,QAAM,IAAI,MAAM,+CAA+C,MAAM;AACvE;AAQA,SAAS,UAAU,MAAM,YAAY;AACnC,UAAQ,MAAI;AAAA,IACZ,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,UAAI,CAAC;AAAY,cAAM,IAAI,MAAM,uCAAuC;AACxE,aAAO;AAAA,IACT;AACE,YAAM,IAAI,MAAM,6BAA6B,MAAM;AAAA,EACpD;AACH;AC/JO,SAAS,UAAU,QAAQ,MAAM,OAAO,aAAa;AAC1D,MAAI,UAAU;AAAG,WAAO,CAAE;AAC1B,MAAI,SAAS,WAAW;AACtB,WAAO,iBAAiB,QAAQ,KAAK;AAAA,EACzC,WAAa,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACvC,WAAa,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACvC,WAAa,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACvC,WAAa,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACvC,WAAa,SAAS,UAAU;AAC5B,WAAO,gBAAgB,QAAQ,KAAK;AAAA,EACxC,WAAa,SAAS,cAAc;AAChC,WAAO,mBAAmB,QAAQ,KAAK;AAAA,EAC3C,WAAa,SAAS,wBAAwB;AAC1C,QAAI,CAAC;AAAa,YAAM,IAAI,MAAM,8BAA8B;AAChE,WAAO,wBAAwB,QAAQ,OAAO,WAAW;AAAA,EAC7D,OAAS;AACL,UAAM,IAAI,MAAM,2BAA2B,MAAM;AAAA,EAClD;AACH;AASA,SAAS,iBAAiB,QAAQ,OAAO;AACvC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,aAAa,OAAO,UAAU,IAAI,IAAI;AAC5C,UAAM,YAAY,IAAI;AACtB,UAAM,OAAO,OAAO,KAAK,SAAS,UAAU;AAC5C,WAAO,MAAM,OAAO,KAAK,eAAe;AAAA,EACzC;AACD,SAAO,UAAU,KAAK,KAAK,QAAQ,CAAC;AACpC,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,WAAW,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC3F,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACpF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,cAAc,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC9F,IAAI,cAAc,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACvF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,MAAM,OAAO,KAAK,YAAY,OAAO,SAAS,IAAI,IAAI,IAAI;AAChE,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,SAAS,IAAI,KAAK,GAAG,IAAI;AAClE,WAAO,KAAK,OAAO,IAAI,KAAK,MAAM;AAAA,EACnC;AACD,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,aAAa,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC7F,IAAI,aAAa,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACtF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,gBAAgB,QAAQ,OAAO;AACtC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,aAAa,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC7F,IAAI,aAAa,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACtF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,mBAAmB,QAAQ,OAAO;AACzC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,SAAS,OAAO,KAAK,UAAU,OAAO,QAAQ,IAAI;AACxD,WAAO,UAAU;AACjB,WAAO,KAAK,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,MAAM;AAC7F,WAAO,UAAU;AAAA,EAClB;AACD,SAAO;AACT;AAUA,SAAS,wBAAwB,QAAQ,OAAO,aAAa;AAE3D,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,KAAK,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,WAAW;AAClG,WAAO,UAAU;AAAA,EAClB;AACD,SAAO;AACT;AAWA,SAAS,MAAM,QAAQ,QAAQ,MAAM;AACnC,QAAM,UAAU,IAAI,YAAY,IAAI;AACpC,MAAI,WAAW,OAAO,EAAE,IAAI,IAAI,WAAW,QAAQ,QAAQ,IAAI,CAAC;AAChE,SAAO;AACT;AC7KA,MAAM,YAAY,CAAC,GAAG,KAAM,OAAQ,UAAU,UAAU;AAWxD,SAAS,UAAU,WAAW,SAAS,SAAS,OAAO,QAAQ;AAC7D,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,YAAQ,QAAQ,KAAK,UAAU,UAAU;AAAA,EAC1C;AACH;AASO,SAAS,iBAAiB,OAAO,QAAQ;AAC9C,QAAM,cAAc,MAAM;AAC1B,QAAM,eAAe,OAAO;AAC5B,MAAI,MAAM;AACV,MAAI,SAAS;AAGb,SAAO,MAAM,aAAa;AACxB,UAAM,IAAI,MAAM;AAChB;AACA,QAAI,IAAI,KAAK;AACX;AAAA,IACD;AAAA,EACF;AACD,MAAI,gBAAgB,OAAO,aAAa;AACtC,UAAM,IAAI,MAAM,8BAA8B;AAAA,EAC/C;AAED,SAAO,MAAM,aAAa;AACxB,UAAM,IAAI,MAAM;AAChB,QAAI,MAAM;AACV;AAEA,QAAI,OAAO,aAAa;AACtB,YAAM,IAAI,MAAM,oBAAoB;AAAA,IACrC;AAGD,SAAK,IAAI,OAAS,GAAG;AAEnB,UAAIC,QAAO,MAAM,KAAK;AAEtB,UAAIA,OAAM,IAAI;AACZ,YAAI,MAAM,KAAK,aAAa;AAC1B,gBAAM,IAAI,MAAM,6CAA6C;AAAA,QAC9D;AACD,cAAM,aAAaA,OAAM;AACzB,QAAAA,OAAM,MAAM,QACP,MAAM,MAAM,MAAM,MAClB,MAAM,MAAM,MAAM,OAClB,MAAM,MAAM,MAAM;AACvB,QAAAA,QAAOA,OAAM,UAAU,eAAe;AACtC,eAAO;AAAA,MACR;AACD,UAAI,MAAMA,OAAM,aAAa;AAC3B,cAAM,IAAI,MAAM,2CAA2C;AAAA,MAC5D;AACD,gBAAU,OAAO,KAAK,QAAQ,QAAQA,IAAG;AACzC,aAAOA;AACP,gBAAUA;AAAA,IAChB,OAAW;AAEL,UAAI,SAAS;AACb,cAAQ,IAAI,GAAG;AAAA,QACf,KAAK;AAEH,iBAAO,MAAM,IAAI,KAAO;AACxB,mBAAS,MAAM,QAAQ,MAAM,KAAK;AAClC;AACA;AAAA,QACF,KAAK;AAEH,cAAI,eAAe,MAAM,GAAG;AAC1B,kBAAM,IAAI,MAAM,2BAA2B;AAAA,UAC5C;AACD,iBAAO,MAAM,KAAK;AAClB,mBAAS,MAAM,QAAQ,MAAM,MAAM,MAAM;AACzC,iBAAO;AACP;AAAA,QACF,KAAK;AAEH,cAAI,eAAe,MAAM,GAAG;AAC1B,kBAAM,IAAI,MAAM,2BAA2B;AAAA,UAC5C;AACD,iBAAO,MAAM,KAAK;AAClB,mBAAS,MAAM,QACV,MAAM,MAAM,MAAM,MAClB,MAAM,MAAM,MAAM,OAClB,MAAM,MAAM,MAAM;AACvB,iBAAO;AACP;AAAA,MAGD;AACD,UAAI,WAAW,KAAK,MAAM,MAAM,GAAG;AACjC,cAAM,IAAI,MAAM,kBAAkB,cAAc,mBAAmB,aAAa;AAAA,MACjF;AACD,UAAI,SAAS,QAAQ;AACnB,cAAM,IAAI,MAAM,yCAAyC;AAAA,MAC1D;AACD,gBAAU,QAAQ,SAAS,QAAQ,QAAQ,QAAQ,GAAG;AACtD,gBAAU;AAAA,IACX;AAAA,EACF;AAED,MAAI,WAAW;AAAc,UAAM,IAAI,MAAM,wBAAwB;AACvE;AChHO,SAAS,aAAa,OAAO,MAAM,EAAE,MAAM,SAAS,cAAc;AACvE,QAAM,OAAO,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU;AAC1E,QAAM,SAAS,EAAE,MAAM,QAAQ,EAAG;AAElC,MAAI;AAGJ,QAAM,mBAAmB,qBAAqB,QAAQ,MAAM,UAAU;AAEtE,QAAM,EAAE,kBAAkB,SAAU,IAAG,qBAAqB,QAAQ,MAAM,UAAU;AAIpF,QAAM,UAAU,KAAK,aAAa;AAClC,MAAI,KAAK,aAAa,SAAS;AAC7B,eAAW,UAAU,QAAQ,MAAM,SAAS,QAAQ,WAAW;AAAA,EACnE,WACI,KAAK,aAAa,sBAClB,KAAK,aAAa,oBAClB,KAAK,aAAa,OAClB;AACA,UAAMD,YAAW,SAAS,YAAY,IAAI,KAAK,SAAS,OAAO,QAAQ;AACvE,QAAIA,WAAU;AACZ,iBAAW,IAAI,MAAM,OAAO;AAC5B,UAAI,SAAS,WAAW;AACtB,+BAAuB,QAAQA,WAAU,QAAQ;AACjD,mBAAW,SAAS,IAAI,OAAK,CAAC,CAAC,CAAC;AAAA,MACxC,OAAa;AAEL,+BAAuB,QAAQA,WAAU,UAAU,KAAK,aAAa,OAAO,MAAM;AAAA,MACnF;AAAA,IACP,OAAW;AACL,iBAAW,IAAI,WAAW,OAAO;AAAA,IAClC;AAAA,EACL,WAAa,KAAK,aAAa,qBAAqB;AAChD,eAAW,gBAAgB,QAAQ,SAAS,MAAM,QAAQ,WAAW;AAAA,EACzE,WAAa,KAAK,aAAa,uBAAuB;AAClD,UAAM,QAAQ,SAAS;AACvB,eAAW,QAAQ,IAAI,WAAW,OAAO,IAAI,IAAI,cAAc,OAAO;AACtE,sBAAkB,QAAQ,SAAS,QAAQ;AAAA,EAC/C,WAAa,KAAK,aAAa,2BAA2B;AACtD,eAAW,IAAI,MAAM,OAAO;AAC5B,yBAAqB,QAAQ,SAAS,QAAQ;AAAA,EAClD,OAAS;AACL,UAAM,IAAI,MAAM,iCAAiC,KAAK,UAAU;AAAA,EACjE;AAED,SAAO,EAAE,kBAAkB,kBAAkB,SAAU;AACzD;AASA,SAAS,qBAAqB,QAAQ,MAAM,YAAY;AACtD,MAAI,WAAW,SAAS,GAAG;AACzB,UAAM,qBAAqB,sBAAsB,UAAU;AAC3D,QAAI,oBAAoB;AACtB,YAAM,SAAS,IAAI,MAAM,KAAK,UAAU;AACxC,6BAAuB,QAAQ,SAAS,kBAAkB,GAAG,MAAM;AACnE,aAAO;AAAA,IACR;AAAA,EACF;AACD,SAAO,CAAE;AACX;AAQA,SAAS,qBAAqB,QAAQ,MAAM,YAAY;AACtD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,CAAC;AAAoB,WAAO,EAAE,kBAAkB,CAAE,GAAE,UAAU,EAAG;AAErE,QAAM,mBAAmB,IAAI,MAAM,KAAK,UAAU;AAClD,yBAAuB,QAAQ,SAAS,kBAAkB,GAAG,gBAAgB;AAG7E,MAAI,WAAW,KAAK;AACpB,aAAW,OAAO,kBAAkB;AAClC,QAAI,QAAQ;AAAoB;AAAA,EACjC;AACD,MAAI,aAAa;AAAG,qBAAiB,SAAS;AAE9C,SAAO,EAAE,kBAAkB,SAAU;AACvC;AASO,SAAS,eAAe,iBAAiB,wBAAwB,OAAOE,cAAa;AAE1F,MAAI;AACJ,QAAM,qBAAqBA,eAAc;AACzC,MAAI,UAAU,gBAAgB;AAC5B,WAAO;AAAA,EACR,WAAU,oBAAoB;AAC7B,WAAO,mBAAmB,iBAAiB,sBAAsB;AAAA,EACrE,WAAa,UAAU,UAAU;AAC7B,WAAO,IAAI,WAAW,sBAAsB;AAC5C,qBAAiB,iBAAiB,IAAI;AAAA,EAC1C,OAAS;AACL,UAAM,IAAI,MAAM,0CAA0C,OAAO;AAAA,EAClE;AACD,MAAI,MAAM,WAAW,wBAAwB;AAC3C,UAAM,IAAI,MAAM,oCAAoC,MAAM,gCAAgC,wBAAwB;AAAA,EACnH;AACD,SAAO;AACT;AAWO,SAAS,eAAe,iBAAiB,IAAI,eAAe;AACjE,QAAM,OAAO,IAAI,SAAS,gBAAgB,QAAQ,gBAAgB,YAAY,gBAAgB,UAAU;AACxG,QAAM,SAAS,EAAE,MAAM,QAAQ,EAAG;AAClC,QAAM,EAAE,MAAM,SAAS,YAAY,OAAO,aAAAA,aAAW,IAAK;AAC1D,QAAM,QAAQ,GAAG;AACjB,MAAI,CAAC;AAAO,UAAM,IAAI,MAAM,0CAA0C;AAGtE,QAAM,mBAAmB,uBAAuB,QAAQ,OAAO,UAAU;AACzE,SAAO,SAAS,MAAM;AAGtB,QAAM,mBAAmB,uBAAuB,QAAQ,OAAO,UAAU;AAGzE,QAAM,uBAAuB,GAAG,yBAAyB,MAAM,gCAAgC,MAAM;AAErG,MAAI,OAAO,gBAAgB,SAAS,OAAO,MAAM;AACjD,MAAI,MAAM,kBAAkB,OAAO;AACjC,WAAO,eAAe,MAAM,sBAAsB,OAAOA,YAAW;AAAA,EACrE;AACD,QAAM,WAAW,IAAI,SAAS,KAAK,QAAQ,KAAK,YAAY,KAAK,UAAU;AAC3E,QAAM,aAAa,EAAE,MAAM,UAAU,QAAQ,EAAG;AAIhD,MAAI;AACJ,QAAM,UAAU,MAAM,aAAa,MAAM;AACzC,MAAI,MAAM,aAAa,SAAS;AAC9B,eAAW,UAAU,YAAY,MAAM,SAAS,QAAQ,WAAW;AAAA,EACvE,WAAa,MAAM,aAAa,OAAO;AAEnC,eAAW,IAAI,MAAM,OAAO;AAC5B,2BAAuB,YAAY,GAAG,QAAQ;AAC9C,eAAW,SAAS,IAAI,OAAK,CAAC,CAAC,CAAC;AAAA,EACpC,WACI,MAAM,aAAa,sBACnB,MAAM,aAAa,kBACnB;AACA,UAAMF,YAAW,SAAS,SAAS,WAAW,QAAQ;AACtD,eAAW,IAAI,MAAM,OAAO;AAC5B,2BAAuB,YAAYA,WAAU,UAAU,uBAAuB,CAAC;AAAA,EACnF,WAAa,MAAM,aAAa,uBAAuB;AACnD,UAAM,QAAQ,SAAS;AACvB,eAAW,QAAQ,IAAI,WAAW,OAAO,IAAI,IAAI,cAAc,OAAO;AACtE,sBAAkB,YAAY,SAAS,QAAQ;AAAA,EACnD,WAAa,MAAM,aAAa,2BAA2B;AACvD,eAAW,IAAI,MAAM,OAAO;AAC5B,yBAAqB,YAAY,SAAS,QAAQ;AAAA,EACtD,WAAa,MAAM,aAAa,oBAAoB;AAChD,eAAW,IAAI,MAAM,OAAO;AAC5B,mBAAe,YAAY,SAAS,QAAQ;AAAA,EAChD,WAAa,MAAM,aAAa,qBAAqB;AACjD,eAAW,gBAAgB,YAAY,SAAS,MAAM,QAAQ,WAAW;AAAA,EAC7E,OAAS;AACL,UAAM,IAAI,MAAM,iCAAiC,MAAM,UAAU;AAAA,EAClE;AAED,SAAO,EAAE,kBAAkB,kBAAkB,SAAU;AACzD;AAQA,SAAS,uBAAuB,QAAQ,OAAO,YAAY;AACzD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,CAAC;AAAoB,WAAO,CAAE;AAElC,QAAM,SAAS,IAAI,MAAM,MAAM,UAAU;AACzC,yBAAuB,QAAQ,SAAS,kBAAkB,GAAG,QAAQ,MAAM,6BAA6B;AACxG,SAAO;AACT;AAQA,SAAS,uBAAuB,QAAQ,OAAO,YAAY;AACzD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,oBAAoB;AAEtB,UAAM,SAAS,IAAI,MAAM,MAAM,UAAU;AACzC,2BAAuB,QAAQ,SAAS,kBAAkB,GAAG,QAAQ,MAAM,6BAA6B;AACxG,WAAO;AAAA,EACR;AACH;ACvNO,SAAS,WAAW,QAAQ,EAAE,YAAY,aAAa,UAAW,GAAE,eAAe,QAAQ;AAChG,QAAM,EAAE,cAAc,WAAU,IAAK;AACrC,QAAM,SAAS,aAAa,UAAU;AAEtC,QAAM,SAAS,CAAE;AAEjB,MAAI,aAAa;AAEjB,MAAI,YAAY;AAChB,MAAI,WAAW;AAEf,QAAM,gBAAgB,WAAW,MAAM;AACrC,iBAAa,OAAO;AAAA,MAClB;AAAA,MACA,YAAY;AAAA,MACZ,UAAU,aAAa,WAAW,UAAU;AAAA,MAC5C,QAAQ,aAAa;AAAA,IAC3B,CAAK;AAAA,EACL;AAEE,SAAO,SAAS,WAAW,YAAY,OAAO,SAAS,OAAO,KAAK,aAAa,GAAG;AACjF,QAAI,OAAO,UAAU,OAAO,KAAK,aAAa;AAAG;AAGjD,UAAM,SAAS,cAAc,MAAM;AACnC,QAAI,OAAO,SAAS,mBAAmB;AAErC,mBAAa,SAAS,QAAQ,QAAQ,eAAe,YAAY,QAAW,CAAC;AAC7E,mBAAa,QAAQ,YAAY,aAAa;AAAA,IACpD,OAAW;AACL,YAAM,kBAAkB,WAAW,UAAU;AAC7C,YAAM,SAAS,SAAS,QAAQ,QAAQ,eAAe,YAAY,WAAW,cAAc,QAAQ;AACpG,UAAI,cAAc,QAAQ;AAExB,oBAAY,OAAO,SAAS;AAAA,MACpC,OAAa;AACL,wBAAiB;AACjB,eAAO,KAAK,MAAM;AAClB,oBAAY,OAAO;AACnB,oBAAY;AAAA,MACb;AAAA,IACF;AAAA,EACF;AACD,kBAAiB;AAEjB,MAAI,WAAW,aAAa,WAAW;AAErC,WAAO,OAAO,SAAS,KAAK,UAAU,MAAM,GAAG,aAAa,WAAW,UAAU,OAAO;AAAA,EACzF;AACD,SAAO;AACT;AAaO,SAAS,SAAS,QAAQ,QAAQ,eAAe,YAAY,eAAe,WAAW;AAC5F,QAAM,EAAE,MAAM,SAAS,YAAY,OAAO,aAAAE,aAAW,IAAK;AAE1D,QAAM,kBAAkB,IAAI;AAAA,IAC1B,OAAO,KAAK;AAAA,IAAQ,OAAO,KAAK,aAAa,OAAO;AAAA,IAAQ,OAAO;AAAA,EACpE;AACD,SAAO,UAAU,OAAO;AAGxB,MAAI,OAAO,SAAS,aAAa;AAC/B,UAAM,OAAO,OAAO;AACpB,QAAI,CAAC;AAAM,YAAM,IAAI,MAAM,uCAAuC;AAGlE,QAAI,YAAY,KAAK,cAAc,aAAa,UAAU,GAAG;AAC3D,aAAO,IAAI,MAAM,KAAK,UAAU;AAAA,IACjC;AAED,UAAM,OAAO,eAAe,iBAAiB,OAAO,OAAO,sBAAsB,GAAG,OAAOA,YAAW;AACtG,UAAM,EAAE,kBAAkB,kBAAkB,SAAQ,IAAK,aAAa,MAAM,MAAM,aAAa;AAI/F,QAAI,SAAS,sBAAsB,UAAU,YAAY,KAAK,UAAU,aAAa;AACrF,QAAI,iBAAiB,UAAU,kBAAkB,QAAQ;AACvD,YAAM,SAAS,MAAM,QAAQ,aAAa,IAAI,gBAAgB,CAAE;AAChE,aAAO,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,UAAU;AAAA,IACzF,OAAW;AAEL,eAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,KAAK;AAC1C,YAAI,WAAW,GAAG,QAAQ,oBAAoB,YAAY;AACxD,mBAAS,MAAM,KAAK,QAAQ,OAAK,CAAC,CAAC,CAAC;AAAA,QACrC;AAAA,MACF;AACD,aAAO;AAAA,IACR;AAAA,EACL,WAAa,OAAO,SAAS,gBAAgB;AACzC,UAAM,QAAQ,OAAO;AACrB,QAAI,CAAC;AAAO,YAAM,IAAI,MAAM,0CAA0C;AAGtE,QAAI,YAAY,MAAM,UAAU;AAC9B,aAAO,IAAI,MAAM,MAAM,UAAU;AAAA,IAClC;AAED,UAAM,EAAE,kBAAkB,kBAAkB,SAAU,IACpD,eAAe,iBAAiB,QAAQ,aAAa;AAGvD,UAAM,SAAS,sBAAsB,UAAU,YAAY,MAAM,UAAU,aAAa;AACxF,UAAM,SAAS,MAAM,QAAQ,aAAa,IAAI,gBAAgB,CAAE;AAChE,WAAO,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,UAAU;AAAA,EACvF,WAAa,OAAO,SAAS,mBAAmB;AAC5C,UAAM,OAAO,OAAO;AACpB,QAAI,CAAC;AAAM,YAAM,IAAI,MAAM,6CAA6C;AAExE,UAAM,OAAO;AAAA,MACX;AAAA,MAAiB,OAAO,OAAO,sBAAsB;AAAA,MAAG;AAAA,MAAOA;AAAA,IAChE;AAED,UAAMC,UAAS,EAAE,MAAM,IAAI,SAAS,KAAK,QAAQ,KAAK,YAAY,KAAK,UAAU,GAAG,QAAQ,EAAG;AAC/F,WAAO,UAAUA,SAAQ,MAAM,KAAK,YAAY,QAAQ,WAAW;AAAA,EACvE,OAAS;AACL,UAAM,IAAI,MAAM,kCAAkC,OAAO,MAAM;AAAA,EAChE;AACH;AASA,SAAS,cAAc,QAAQ;AAC7B,QAAM,SAAS,4BAA4B,MAAM;AAGjD,QAAM,OAAO,UAAU,OAAO;AAC9B,QAAM,yBAAyB,OAAO;AACtC,QAAM,uBAAuB,OAAO;AACpC,QAAM,MAAM,OAAO;AACnB,QAAM,mBAAmB,OAAO,WAAW;AAAA,IACzC,YAAY,OAAO,QAAQ;AAAA,IAC3B,UAAU,UAAU,OAAO,QAAQ;AAAA,IACnC,2BAA2B,UAAU,OAAO,QAAQ;AAAA,IACpD,2BAA2B,UAAU,OAAO,QAAQ;AAAA,IACpD,YAAY,OAAO,QAAQ,WAAW;AAAA,MACpC,KAAK,OAAO,QAAQ,QAAQ;AAAA,MAC5B,KAAK,OAAO,QAAQ,QAAQ;AAAA,MAC5B,YAAY,OAAO,QAAQ,QAAQ;AAAA,MACnC,gBAAgB,OAAO,QAAQ,QAAQ;AAAA,MACvC,WAAW,OAAO,QAAQ,QAAQ;AAAA,MAClC,WAAW,OAAO,QAAQ,QAAQ;AAAA,IACnC;AAAA,EACF;AACD,QAAM,oBAAoB,OAAO;AACjC,QAAM,yBAAyB,OAAO,WAAW;AAAA,IAC/C,YAAY,OAAO,QAAQ;AAAA,IAC3B,UAAU,UAAU,OAAO,QAAQ;AAAA,IACnC,WAAW,OAAO,QAAQ;AAAA,EAC3B;AACD,QAAM,sBAAsB,OAAO,WAAW;AAAA,IAC5C,YAAY,OAAO,QAAQ;AAAA,IAC3B,WAAW,OAAO,QAAQ;AAAA,IAC1B,UAAU,OAAO,QAAQ;AAAA,IACzB,UAAU,UAAU,OAAO,QAAQ;AAAA,IACnC,+BAA+B,OAAO,QAAQ;AAAA,IAC9C,+BAA+B,OAAO,QAAQ;AAAA,IAC9C,eAAe,OAAO,QAAQ,YAAY,SAAY,OAAO,OAAO,QAAQ;AAAA,IAC5E,YAAY,OAAO,QAAQ;AAAA,EAC5B;AAED,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACD;AACH;ACzLO,SAAS,aAAa,SAAS,EAAE,UAAU,QAAO,GAAI,WAAW;AACtE,QAAM,EAAE,MAAM,aAAAD,cAAa,KAAM,IAAG;AAGpC,QAAM,eAAe,CAAE;AAEvB,QAAM,UAAU,EAAE,GAAG,iBAAiB,GAAG,QAAQ,QAAS;AAG1D,aAAW,EAAE,WAAW,UAAW,KAAI,UAAU,SAAS,SAAS;AACjE,QAAI;AAAW,YAAM,IAAI,MAAM,iCAAiC;AAChE,QAAI,CAAC;AAAW,YAAM,IAAI,MAAM,sCAAsC;AAGtE,UAAM,aAAa,UAAU,eAAe;AAC5C,QAAI,WAAW,CAAC,QAAQ,SAAS,UAAU;AAAG;AAE9C,UAAM,EAAE,WAAW,YAAY,eAAe,SAAS;AACvD,UAAM,cAAc,UAAU;AAI9B,QAAI,cAAc,KAAK,IAAI;AACzB,cAAQ,KAAK,iCAAiC,UAAU,mBAAmB,mBAAmB;AAE9F;AAAA,IACD;AAID,UAAM,SAAS,QAAQ,QAAQ,KAAK,MAAM,WAAW,OAAO,CAAC;AAG7D,iBAAa,KAAK;AAAA,MAChB,cAAc,UAAU;AAAA,MACxB,MAAM,OAAO,KAAK,iBAAe;AAC/B,cAAM,aAAa,cAAc,SAAS,QAAQ,UAAU,cAAc;AAC1E,cAAM,SAAS,EAAE,MAAM,IAAI,SAAS,WAAW,GAAG,QAAQ,EAAG;AAC7D,cAAM,gBAAgB;AAAA,UACpB,cAAc,UAAU;AAAA,UACxB,MAAM,UAAU;AAAA,UAChB,SAAS,WAAW,WAAW,SAAS,GAAG;AAAA,UAC3C;AAAA,UACA,OAAO,UAAU;AAAA,UACjB;AAAA,UACA,aAAAA;AAAA,UACA;AAAA,QACD;AACD,eAAO,WAAW,QAAQ,WAAW,eAAe,QAAQ,MAAM;AAAA,MAC1E,CAAO;AAAA,IACP,CAAK;AAAA,EACF;AAED,SAAO,EAAE,YAAY,UAAU,YAAY,WAAW,UAAU,WAAW,aAAc;AAC3F;AA4BO,eAAe,iBAAiB,EAAE,aAAc,GAAE,aAAa,WAAW,SAAS,WAAW;AAGnG,QAAM,cAAc,MAAM,QAAQ,IAAI,aAAa,IAAI,CAAC,EAAE,KAAM,MAAK,KAAK,KAAK,OAAO,CAAC,CAAC;AAGxF,QAAM,sBAAsB,aACzB,IAAI,WAAS,MAAM,aAAa,EAAE,EAClC,OAAO,UAAQ,CAAC,WAAW,QAAQ,SAAS,IAAI,CAAC;AACpD,QAAM,cAAc,WAAW;AAC/B,QAAM,gBAAgB,YAAY,IAAI,UAAQ,aAAa,UAAU,YAAU,OAAO,aAAa,OAAO,IAAI,CAAC;AAG/G,QAAM,cAAc,YAAY;AAChC,MAAI,cAAc,UAAU;AAE1B,UAAME,aAAY,IAAI,MAAM,WAAW;AACvC,aAAS,YAAY,GAAG,YAAY,aAAa,aAAa;AAC5D,YAAM,MAAM,cAAc;AAG1B,YAAM,UAAU,CAAE;AAClB,eAAS,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK;AAC5C,gBAAQ,aAAa,GAAG,aAAa,MAAM,YAAY,GAAG;AAAA,MAC3D;AACD,MAAAA,WAAU,aAAa;AAAA,IACxB;AACD,WAAOA;AAAA,EACR;AAGD,QAAM,YAAY,IAAI,MAAM,WAAW;AACvC,WAAS,YAAY,GAAG,YAAY,aAAa,aAAa;AAC5D,UAAM,MAAM,cAAc;AAE1B,UAAM,UAAU,IAAI,MAAM,aAAa,MAAM;AAC7C,aAAS,IAAI,GAAG,IAAI,YAAY,QAAQ,KAAK;AAC3C,UAAI,cAAc,MAAM,GAAG;AACzB,gBAAQ,KAAK,YAAY,cAAc,IAAI;AAAA,MAC5C;AAAA,IACF;AACD,cAAU,aAAa;AAAA,EACxB;AACD,SAAO;AACT;AASO,SAAS,cAAc,eAAeN,aAAY;AACvD,QAAM,EAAE,aAAY,IAAK;AAEzB,QAAM,YAAY,CAAE;AACpB,aAAW,SAASA,YAAW,UAAU;AACvC,QAAI,MAAM,SAAS,QAAQ;AACzB,YAAM,eAAe,aAAa,OAAO,YAAU,OAAO,aAAa,OAAO,MAAM,QAAQ,IAAI;AAChG,UAAI,CAAC,aAAa;AAAQ;AAI1B,YAAM,WAAW,oBAAI,IAAK;AAC1B,YAAM,OAAO,QAAQ,IAAI,aAAa,IAAI,YAAU;AAClD,eAAO,OAAO,KAAK,KAAK,gBAAc;AACpC,mBAAS,IAAI,OAAO,aAAa,KAAK,GAAG,GAAG,QAAQ,UAAU,CAAC;AAAA,QACzE,CAAS;AAAA,MACT,CAAO,CAAC,EAAE,KAAK,MAAM;AAEb,uBAAe,UAAU,KAAK;AAC9B,cAAM,aAAa,SAAS,IAAI,MAAM,KAAK,KAAK,GAAG,CAAC;AACpD,YAAI,CAAC;AAAY,gBAAM,IAAI,MAAM,mCAAmC;AACpE,eAAO,CAAC,UAAU;AAAA,MAC1B,CAAO;AAED,gBAAU,KAAK,EAAE,cAAc,MAAM,MAAM,MAAM;AAAA,IACvD,OAAW;AAEL,YAAM,cAAc,aAAa,KAAK,YAAU,OAAO,aAAa,OAAO,MAAM,QAAQ,IAAI;AAC7F,UAAI,aAAa;AACf,kBAAU,KAAK,WAAW;AAAA,MAC3B;AAAA,IACF;AAAA,EACF;AACD,SAAO,EAAE,GAAG,eAAe,cAAc,UAAW;AACtD;ACvKO,eAAe,YAAY,SAAS;AAEzC,UAAQ,aAAa,MAAM,qBAAqB,QAAQ,MAAM,OAAO;AAGrE,QAAM,cAAc,iBAAiB,OAAO;AAE5C,QAAM,EAAE,WAAW,GAAG,QAAQ,SAAS,SAAS,YAAY,UAAS,IAAK;AAG1E,MAAI,CAAC,cAAc,CAAC,SAAS;AAC3B,eAAW,EAAE,aAAc,KAAI,aAAa;AAC1C,iBAAW,EAAE,UAAU;AAAc,cAAM;AAAA,IAC5C;AACD;AAAA,EACD;AAGD,QAAMA,cAAa,cAAc,QAAQ,QAAQ;AACjD,QAAM,YAAY,YAAY,IAAI,SAAO,cAAc,KAAKA,WAAU,CAAC;AAGvE,MAAI,SAAS;AACX,eAAW,cAAc,WAAW;AAClC,iBAAW,eAAe,WAAW,cAAc;AACjD,oBAAY,KAAK,KAAK,iBAAe;AACnC,cAAIO,YAAW,WAAW;AAC1B,qBAAW,cAAc,aAAa;AACpC,oBAAQ;AAAA,cACN,YAAY,YAAY,aAAa;AAAA,cACrC;AAAA,cACA,UAAAA;AAAA,cACA,QAAQA,YAAW,WAAW;AAAA,YAC5C,CAAa;AACD,YAAAA,aAAY,WAAW;AAAA,UACxB;AAAA,QACX,CAAS;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAGD,MAAI,YAAY;AAGd,UAAM,OAAO,CAAE;AACf,eAAW,cAAc,WAAW;AAElC,YAAM,cAAc,KAAK,IAAI,WAAW,WAAW,YAAY,CAAC;AAChE,YAAM,YAAY,KAAK,KAAK,UAAU,YAAY,WAAW,YAAY,WAAW,SAAS;AAE7F,YAAM,YAAY,cAAc,WAC9B,MAAM,iBAAiB,YAAY,aAAa,WAAW,SAAS,QAAQ,IAC5E,MAAM,iBAAiB,YAAY,aAAa,WAAW,SAAS,OAAO;AAC7E,aAAO,MAAM,SAAS;AAAA,IACvB;AACD,eAAW,IAAI;AAAA,EACnB,OAAS;AAEL,eAAW,EAAE,aAAc,KAAI,WAAW;AACxC,iBAAW,EAAE,UAAU;AAAc,cAAM;AAAA,IAC5C;AAAA,EACF;AACH;AAMO,SAAS,iBAAiB,SAAS;AACxC,MAAI,CAAC,QAAQ;AAAU,UAAM,IAAI,MAAM,2BAA2B;AAIlE,QAAM,OAAO,YAAY,OAAO;AAChC,UAAQ,OAAO,oBAAoB,QAAQ,MAAM,IAAI;AAGrD,SAAO,KAAK,OAAO,IAAI,eAAa,aAAa,SAAS,MAAM,SAAS,CAAC;AAC5E;AChGO,SAAS,mBAAmB,KACnC;AACE,UAAQ,KAAK,yBAAyB;AAChC,QAAA,OAAO,IAAIC;AAEX,QAAA,cAAc,IAAI,QAAQ;AAC1B,QAAA,aAAa,IAAI,YAAY;AAC7B,QAAA,YAAY,IAAI,iBAAiB;AACjC,QAAA,WAAW,IAAI,YAAY;AAC3B,QAAA,eAAe,IAAI,iBAAiB;AACpC,QAAA,iBAAiB,IAAI,YAAY;AAE/B,UAAA,IAAI,EAAE,aAAa,YAAY,WAAW,UAAU,cAAc,gBAAgB;AAEpF,QAAA,oBAAoB,kBAAkB,GAAG;AACzC,QAAA,iBAAiB,sBAAsB,GAAG;AAC1C,QAAA,YAAY,iBAAiB,GAAG;AAGtC,UAAQ,KAAK,mBAAmB;AAO1B,QAAA,8BAAc;AAEpB,WAAS,KAAK,GAAG,KAAK,cAAc,MAAM;AAClC,UAAA,YAAY,IAAI,iBAAiB;AACjC,UAAA,gBAAgB,IAAI,qBAAqB;AAE3C,QAAA,YAAY,KAAK,aAAa;AAAW;AACzC,QAAA,gBAAgB,KAAK,iBAAiB;AAAU;AACpD,QAAI,CAAC,eAAe;AAAY;AAE1B,UAAA,MAAM,GAAG,aAAa;AACxB,QAAA,SAAS,QAAQ,IAAI,GAAG;AAC5B,QAAI,CAAC,QAAQ;AACX,eAAS,EAAE,WAAW,eAAe,gBAAgB,CAAG,EAAA;AAChD,cAAA,IAAI,KAAK,MAAM;AAAA,IACzB;AACO,WAAA,eAAe,KAAK,EAAE;AAAA,EAC/B;AACA,UAAQ,QAAQ,mBAAmB;AAW7B,QAAA,uCAAuB;AAG7B,UAAQ,KAAK,2BAA2B;AAExC,aAAW,CAAA,EAAG,MAAM,KAAK,SAAS;AAChC,UAAM,EAAE,WAAW,eAAe,eAAA,IAAmB;AACrD,UAAM,OAAO,eAAe;AAC5B,QAAI,CAAC;AAAM;AAEX,UAAM,WAAW,UAAU;AAC3B,UAAM,QAAQ,eAAe;AAE7B,QAAI,UAAU,GAAG;AACf,YAAM,KAAK,eAAe;AACpB,YAAA,KAAK,IAAI,sBAAsB;AAC/B,YAAA,cAAc,IAAI,mBAAmB,OAAO;AAE9C,UAAA,OAAO,iBAAiB,IAAI,aAAa;AAC7C,UAAI,CAAC,MAAM;AACT,eAAO,CAAA;AACU,yBAAA,IAAI,eAAe,IAAI;AAAA,MAC1C;AAEA,WAAK,KAAK;AAAA,QACR;AAAA,QACA,gBAAgB;AAAA,QAChB;AAAA,QACA;AAAA,MAAA,CACD;AAED;AAAA,IACF;AAEA,UAAM,YAAY,IAAIC,cAAoB,MAAM,UAAU,KAAK;AACrD,cAAA,eAAe,SAASC,eAAqB;AAEvD,aAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,YAAM,KAAK,eAAe;AACpB,YAAA,KAAK,IAAI,sBAAsB;AAC3B,gBAAA,YAAY,GAAG,kBAAkB,GAAG;AAAA,IAChD;AAEC,cAAU,SAAiB,YAAY;AACvC,cAAU,SAAiB,gBAAgB;AAC5C,cAAU,gBAAgB;AAC1B,cAAU,mBAAmB;AAC7B,cAAU,yBAAyB;AACnC,SAAK,IAAI,SAAS;AAAA,EACpB;AAEA,UAAQ,QAAQ,2BAA2B;AAE3C,UAAQ,KAAK,mCAAmC;AAC1C,QAAA,WAAW,IAAIC;AAGrB,aAAW,CAAC,eAAe,OAAO,KAAK,kBACvC;AACM,QAAA,CAAC,WAAW,QAAQ,WAAW;AAAG;AAEtC,UAAM,WAAW,UAAU;AAyDzB,QAAA,QAAQ,WAAW,GAAG;AACxB,YAAM,EAAE,MAAM,eAAe,IAAI,QAAQ;AACzC,YAAM,SAAS,kBAAkB;AAEjC,YAAM,OAAO,IAAIC,KAAW,MAAM,QAAQ;AAC1C,WAAK,mBAAmB;AACnB,WAAA,OAAO,KAAK,MAAM;AACvB,WAAK,IAAI,IAAI;AACb;AAAA,IACF;AAEA,UAAM,eAAuC,CAAA;AAE7C,eAAW,SAAS,SAAS;AACrB,YAAA,EAAE,MAAM,eAAmB,IAAA;AACjC,YAAM,IAAI,kBAAkB;AAGtB,YAAA,IAAI,KAAK;AAEf,UAAI,CAAC,EAAE,OAAO,QAAQ,GAAG;AACvB,UAAE,aAAa,CAAC;AAAA,MAClB;AAEA,mBAAa,KAAK,CAAC;AAAA,IACrB;AAEA,QAAI,aAAa,WAAW;AAAG;AAEzB,UAAA,iBAAiB,gBAAgB,YAAY;AACnD,UAAM,aAAa,IAAIA,KAAW,gBAAgB,QAAQ;AAC1D,eAAW,OAAO,yBAAyB;AAE3C,SAAK,IAAI,UAAU;AAGnB,eAAW,KAAK;AAAc,QAAE,UAAU;AAAA,EAC5C;AAEE,UAAQ,QAAQ,mCAAmC;AAGnD,OAAK,SAAS,IAAI,CAAC,KAAK,KAAK;AAE7B,UAAQ,QAAQ,yBAAyB;AAClC,SAAA;AACT;AAEO,SAAS,kBAAkB,KAClC;AACQ,QAAA;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACE,IAAA;AAEE,QAAA,SAAS,IAAIC;AACb,QAAA,UAAU,IAAIC;AACd,QAAA,WAAW,IAAID;AACrB,QAAM,iBAAiB,YAAY;AAE7B,QAAA,WAAW,IAAI,MAAM,cAAc;AAEzC,WAAS,KAAK,GAAG,KAAK,gBAAgB,MACtC;AACE,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AACvB,UAAM,KAAK,YAAY;AAEjB,UAAA,IAAI,IAAIF;AACP,WAAA,IAAI,IAAI,IAAI,EAAE;AACrB,YAAQ,IAAI,IAAI,IAAI,IAAI,EAAE;AACjB,aAAA,IAAI,IAAI,IAAI,EAAE;AACrB,MAAA,QAAQ,QAAQ,SAAS,QAAQ;AAEnC,aAAS,MAAM;AAAA,EACjB;AACO,SAAA;AACT;AAEO,SAAS,gBACd,YAEF;AAEE,MAAI,aAAa;AACjB,MAAI,WAAW;AAEf,WAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,IAAI,GAAG,KAAK;AACjD,UAAM,WAAW,WAAW;AAEtB,UAAA,QAAQ,SAAS;AACjB,UAAA,WAAW,SAAS,aAAa,UAAU;AAEjD,QAAI,CAAC;AAAa,YAAA,IAAI,MAAM,+CAA+C;AAC3E,QAAI,CAAC;AAAgB,YAAA,IAAI,MAAM,qDAAqD;AAEpF,kBAAc,MAAM;AACpB,gBAAY,SAAS;AAAA,EACvB;AAIA,QAAM,kBAAkB,IAAI,aAAa,WAAW,CAAC;AAC/C,QAAA,gBAAgB,IAAI,YAAY,UAAU;AAEhD,MAAI,cAAc;AAClB,MAAI,eAAe;AAGnB,WAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,IAAI,GAAG,KAAK;AACjD,UAAM,WAAW,WAAW;AAEtB,UAAA,UAAU,SAAS,aAAa,UAAU;AAC1C,UAAA,YAAY,SAAS;AAE3B,UAAM,cAAc,QAAQ;AAC5B,UAAM,gBAAgB,UAAU;AAEhC,UAAM,YAAY,QAAQ;AAC1B,UAAM,WAAW,UAAU;AAE3B,UAAM,cAAc,QAAQ;AAI5B,UAAM,eAAe,YAAY;AACjC,UAAM,eAAe,eAAe;AACpC,oBAAgB,IAAI,YAAY,SAAS,GAAG,YAAY,GAAG,YAAY;AAGvE,aAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACnB,oBAAA,cAAc,KAAK,cAAc,KAAK;AAAA,IACtD;AAEgB,oBAAA;AACD,mBAAA;AAAA,EACjB;AAGM,QAAA,aAAa,IAAII;AACvB,aAAW,aAAa,YAAY,IAAIC,gBAAsB,iBAAiB,CAAC,CAAC;AACjF,aAAW,SAAS,IAAIA,gBAAsB,eAAe,CAAC,CAAC;AACxD,SAAA;AACT;AAEA,SAAS,sBAAsB,KAC/B;AACQ,QAAA,YAAY,IAAI,iBAAiB;AACjC,QAAA,aAAa,IAAI,YAAY;AAC7B,QAAA,cAAc,IAAI,QAAQ;AAC1B,QAAA,iBAA8C,IAAI,MAAM,SAAS;AAEjE,QAAA;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACE,IAAA;AAEJ,WAAS,KAAK,GAAG,KAAK,WAAW,MACjC;AACE,UAAM,SAAS,gBAAgB;AAC/B,UAAM,OAAO,KAAK,IAAI,YAAY,gBAAgB,KAAK,KAAK;AAC5D,UAAM,SAAS,OAAO;AAEtB,UAAM,SAAS,iBAAiB;AAChC,UAAM,OAAO,KAAK,IAAI,YAAY,iBAAiB,KAAK,KAAK;AAC7D,UAAM,SAAS,OAAO;AAElB,QAAA,WAAW,KAAK,WAAW;AAAG;AAElC,UAAM,aAAa,YAAY,SAAS,QAAQ,IAAI;AAEpD,UAAM,mBAAmB;AACzB,UAAM,gBAAgB,IAAI,aAAa,SAAS,CAAC;AACjD,aAAS,KAAK,GAAG,KAAK,QAAQ,MAAM;AAClC,oBAAc,KAAG,IAAE,KAAK,QAAQ,KAAK,UAAU;AAC/C,oBAAc,KAAG,IAAE,KAAK,QAAQ,KAAK,UAAU;AAC/C,oBAAc,KAAG,IAAE,KAAK,QAAQ,KAAK,UAAU;AAAA,IACjD;AAEM,UAAA,OAAO,IAAID;AACjB,SAAK,aAAa,YAAY,IAAIC,gBAAsB,eAAe,CAAC,CAAC;AACzE,SAAK,SAAS,IAAIA,gBAAsB,YAAY,CAAC,CAAC;AACtD,mBAAe,MAAM;AAAA,EACvB;AAEO,SAAA;AACT;AAEA,SAAS,iBAAiB,KAC1B;AACQ,QAAA,eAAe,IAAI,cAAc;AACjC,QAAA,YAAY,IAAI,MAAM,YAAY;AAExC,WAAS,KAAG,GAAG,KAAK,cAAc,MAClC;AACQ,UAAA,IAAI,IAAI,YAAY,MAAM;AAC1B,UAAA,IAAI,IAAI,cAAc,MAAM;AAC5B,UAAA,IAAI,IAAI,aAAa,MAAM;AAC3B,UAAA,IAAI,IAAI,cAAc,MAAO;AAC7B,UAAA,YAAY,IAAI,kBAAkB,MAAM;AACxC,UAAA,YAAY,IAAI,iBAAiB,MAAM;AAEvC,UAAA,MAAM,IAAIC,qBAA2B;AAAA,MACzC,OAAO,IAAIC,MAAY,GAAG,GAAG,CAAC;AAAA,MAC9B,SAAS;AAAA,MACT,aAAa;AAAA,MACb,aAAa,IAAI;AAAA,MACjB;AAAA,MACA;AAAA,MACA,MAAMC;AAAAA,IAAM,CACb;AAED,cAAU,MAAM;AAAA,EAClB;AACO,SAAA;AACT;AChZO,MAAM,oBACb;AAAA,EACE,MAAM,KAAK,QAAsC;AACzC,UAAA,WAAW,MAAM,MAAM,MAAM;AAC/B,QAAA,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,4BAA4B,WAAW,SAAS,UAAU,SAAS,YAAY;AAAA,IACjG;AAEM,UAAA,cAAc,MAAM,SAAS;AACnC,UAAM,MAAM,MAAM,MAAM,UAAU,WAAW;AACvC,UAAA,MAAM,MAAM,uBAAuB,GAAG;AAC5C,WAAO,mBAAmB,GAAG;AAAA,EAC/B;AACF;AAMA,eAAsB,uBAAuB,KAC7C;AAEE,WAAS,mBAAmB,QAAwB;AAC5C,UAAA,cAAc,OAAO;AAC3B,UAAM,OAAO,OAAO,KAAK,IAAI,KAAK,EAAE,KAAK,CAAC,MACxC,EAAE,YAAA,EAAc,SAAS,WAAW,CAAC;AACvC,QAAI,CAAC;AACG,YAAA,IAAI,MAAM,mBAAmB,yBAAyB;AACvD,WAAA;AAAA,EACT;AAGe,iBAAA,iBAAiB,MAAc,WAAgB,MAAW;AACjE,UAAA,YAAY,mBAAmB,IAAI;AACzC,UAAM,OAAO,MAAM,IAAI,MAAM,WAAW,MAAM,aAAa;AACrD,UAAA,WAAW,MAAM,qBAAqB,IAAI;AAChD,UAAM,YAAY,EAAC,MAAM,aAAa,UAAU,QAAQ,OAAmB;AACzE,UAAI,OAAO,MAAM;AACjB,UAAI,KAAK,YAAY,QAAQ,KAAK,MAClC;AAES,eAAA,IAAI,KAAK,IAAI;AAAA,MACtB;AACA,gBAAU,MAAM,cAAc,OAAO,IAAI,KAAK,IAAI,IAAI;AAAA,OACtD;AAAA,EACJ;AAEA,UAAQ,KAAK,wBAAwB;AACrC,QAAM,MAAM,CAAA;AACN,QAAA,iBAAiB,qBAAqB,KAAK,UAAU;AACrD,QAAA,iBAAiB,wBAAwB,KAAK,UAAU;AACxD,QAAA,iBAAiB,uBAAuB,KAAK,WAAW;AACxD,QAAA,iBAAiB,kBAAkB,KAAK,UAAU;AAClD,QAAA,iBAAiB,qBAAqB,KAAK,UAAU;AACrD,QAAA,iBAAiB,sBAAsB,KAAK,YAAY;AAC9D,UAAQ,QAAQ,wBAAwB;AACjC,SAAA;AACT;"}